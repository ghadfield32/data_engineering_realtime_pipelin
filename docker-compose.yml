#location: docker-compose.yml

services:
  #############################################
  # Kafka Stack for Streaming Data
  #############################################
  zookeeper:
    image: docker.io/${ZK_IMAGE}
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "${ZK_PORT}:${ZK_PORT}"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    healthcheck:
      test: echo ruok | nc zookeeper 2181 || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  kafka:
    image: docker.io/${KAFKA_IMAGE}
    hostname: kafka
    container_name: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "${KAFKA_INTERNAL_PORT}:${KAFKA_INTERNAL_PORT}"
      - "${KAFKA_EXTERNAL_PORT}:${KAFKA_EXTERNAL_PORT}"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:9092", "--list"]
      interval: 10s
      timeout: 10s
      retries: 5

  kafka-ui:
    image: docker.io/${KAFKA_UI_IMAGE}
    hostname: kafka-ui
    container_name: kafka-ui
    ports:
      - "${KAFKA_UI_PORT}:8080"
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181

  # Flink JobManager
  flink-jobmanager:
    image: docker.io/${FLINK_IMAGE}
    hostname: jobmanager
    container_name: flink-jobmanager
    ports:
      - "${FLINK_JM_PORT}:8081"
    command: jobmanager
    environment:
      - |  
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager        
        parallelism.default: 1

  # Flink TaskManager
  flink-taskmanager:
    image: docker.io/${FLINK_IMAGE}
    hostname: taskmanager
    container_name: flink-taskmanager
    depends_on:
      - flink-jobmanager
    command: taskmanager
    environment:
      - |  
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager        
        taskmanager.numberOfTaskSlots: 2
        parallelism.default: 1

  #############################################
  # S3-compatible Storage with MinIO
  #############################################
  minio:
    image: docker.io/${MINIO_IMAGE}
    hostname: minio
    container_name: minio
    ports:
      - "${MINIO_API_PORT}:${MINIO_API_PORT}"
      - "${MINIO_CONSOLE_PORT}:${MINIO_CONSOLE_PORT}"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    command: server /data --console-address ":9001"
    volumes:
      - minio-data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Initialize MinIO buckets, users, and policies
  minio-setup:
    image: docker.io/minio/mc
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      mc alias set myminio http://minio:9000 minio minio123;
      mc mb --ignore-existing myminio/gtfs;
      mc mb --ignore-existing myminio/nba;
      mc mb --ignore-existing myminio/weather;
      echo 'MinIO setup completed';
      "

  #############################################
  # DuckDB with MotherDuck Sync
  #############################################
  duckdb:
    # Using community-maintained DuckDB image as there is no official image
    image: docker.io/${DUCKDB_IMAGE}
    hostname: duckdb
    container_name: duckdb
    ports:
      - "${DUCKDB_PORT}:${DUCKDB_PORT}"
    command: ["--remote", "--listen", "0.0.0.0:1294"]
    volumes:
      - duckdb-data:/data
    environment:
      DUCKDB_DATA_DIR: /data

  #############################################
  # Trino coordinator
  #############################################
  trino-coordinator:
    image: docker.io/${TRINO_IMAGE}
    container_name: trino-coordinator
    ports:
      - "${TRINO_PORT}:${TRINO_PORT}"
    volumes:
      - ./trino/catalog:/etc/trino/catalog
      - ./trino/etc:/etc/trino/etc

  #############################################
  # Postgres Database
  #############################################
  postgres:
    image: docker.io/${POSTGRES_IMAGE}
    hostname: postgres
    container_name: postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - "${POSTGRES_PORT}:${POSTGRES_PORT}"
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
      - ./src/infrastructure/docker/init-db.sql:/docker-entrypoint-initdb.d/init-db.sql
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      timeout: 5s
      retries: 5

  #############################################
  # Redis for Airflow CeleryExecutor
  #############################################
  redis:
    image: docker.io/${REDIS_IMAGE}
    hostname: redis
    container_name: redis
    ports:
      - "${REDIS_PORT}:${REDIS_PORT}"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  #############################################
  # Airflow 3.0 with Astro Runtime
  #############################################
  airflow-webserver:
    image: ${AIRFLOW_IMAGE}
    hostname: airflow-webserver
    container_name: airflow-webserver
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      
      # Auth configuration for Airflow 3.0
      AIRFLOW__CORE__AUTH_MANAGER: 'airflow.api_fastapi.auth.managers.simple.simple_auth_manager.SimpleAuthManager'
      AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
      
      # Simple auth manager settings - this is the correct format for Airflow 3.0
      AIRFLOW__SIMPLE_AUTH_MANAGER__USERS: '${AIRFLOW_ADMIN_USERNAME}:${AIRFLOW_ADMIN_PASSWORD}'
      AIRFLOW__SIMPLE_AUTH_MANAGER__ALL_ADMINS: 'True'
      AIRFLOW__SIMPLE_AUTH_MANAGER__PASSWORDS_FILE: '/opt/airflow/simple_auth_manager_passwords.json.generated'
      
      # Database migration flag
      _AIRFLOW_DB_MIGRATE: 'true'
    volumes:
      - ./src/orchestrators/airflow/dags:/opt/airflow/dags
      - ./src/orchestrators/airflow/plugins:/opt/airflow/plugins
      - ./src/orchestrators/airflow/config:/opt/airflow/config
      - airflow-logs-volume:/opt/airflow/logs
      - ./data:/data
      - ./airflow_home/simple_auth_manager_passwords.json.generated:/opt/airflow/simple_auth_manager_passwords.json.generated
    ports:
      - "${AIRFLOW_WEB_PORT}:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/"]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always
    command: api-server

  airflow-scheduler:
    image: ${AIRFLOW_IMAGE}
    hostname: airflow-scheduler
    container_name: airflow-scheduler
    depends_on:
      airflow-webserver:
        condition: service_started
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL: 10
      AIRFLOW__CORE__ENABLE_XCOM_PICKLING: 'true'
      # Enable extra features
      AIRFLOW__CORE__TASK_DEFER_METHOD: 'airflow.triggers.triggerer.DeferralTrigger'
      AIRFLOW__ASSETWATCHER__ENABLE: 'true'
    volumes:
      - ./src/orchestrators/airflow/dags:/opt/airflow/dags
      - ./src/orchestrators/airflow/plugins:/opt/airflow/plugins
      - ./src/orchestrators/dbt:/opt/airflow/dbt
      - airflow-logs-volume:/opt/airflow/logs
      - ./data:/data
    command: scheduler

  airflow-worker:
    image: ${AIRFLOW_IMAGE}
    hostname: airflow-worker
    container_name: airflow-worker
    depends_on:
      airflow-webserver:
        condition: service_started
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      # Enable extra features
      AIRFLOW__CORE__TASK_DEFER_METHOD: 'airflow.triggers.triggerer.DeferralTrigger'
      AIRFLOW__ASSETWATCHER__ENABLE: 'true'
    volumes:
      - ./src/orchestrators/airflow/dags:/opt/airflow/dags
      - ./src/orchestrators/airflow/plugins:/opt/airflow/plugins
      - ./src/orchestrators/dbt:/opt/airflow/dbt
      - airflow-logs-volume:/opt/airflow/logs
      - ./data:/data
    command: celery worker

  airflow-triggerer:
    image: ${AIRFLOW_IMAGE}
    hostname: airflow-triggerer
    container_name: airflow-triggerer
    depends_on:
      airflow-webserver:
        condition: service_started
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      # Enable extra features
      AIRFLOW__CORE__TASK_DEFER_METHOD: 'airflow.triggers.triggerer.DeferralTrigger'
      AIRFLOW__ASSETWATCHER__ENABLE: 'true'
    volumes:
      - ./src/orchestrators/airflow/dags:/opt/airflow/dags
      - ./src/orchestrators/airflow/plugins:/opt/airflow/plugins
      - ./src/orchestrators/dbt:/opt/airflow/dbt
      - airflow-logs-volume:/opt/airflow/logs
      - ./data:/data
    command: triggerer

  #############################################
  # Kestra Open Source Orchestrator
  #############################################
  kestra:
    image: docker.io/${KESTRA_IMAGE}
    hostname: kestra
    container_name: kestra
    ports:
      - "${KESTRA_PORT}:8080"
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      KESTRA_CONFIGURATION: |
        kestra:
          repository:
            type: postgres
            postgres:
              url: jdbc:postgresql://postgres:5432/kestra
              username: airflow
              password: airflow
          queue:
            type: postgres
          storage:
            type: local
            local:
              base-path: "/tmp/storage"
          plugins:
            repositories:
              - id: maven
                type: maven
                url: https://repo.maven.apache.org/maven2/
            locations:
              - io.kestra.plugin:plugin-jdbc:LATEST
              - io.kestra.plugin:plugin-kafka:LATEST
              - io.kestra.plugin:plugin-serdes:LATEST
              - io.kestra.plugin:plugin-notifications:LATEST
              - io.kestra.plugin:plugin-fs-s3:LATEST
    volumes:
      - ./src/orchestrators/kestra/flows:/app/flows
      - ./data:/data

volumes:
  minio-data:
  postgres-db-volume:
  airflow-logs-volume:
  duckdb-data: 
