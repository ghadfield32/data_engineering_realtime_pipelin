{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src\\infrastructure\\docker\\airflow.Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile src\\infrastructure\\docker\\airflow.Dockerfile\n",
    "FROM apache/airflow:3.0.1-python3.10\n",
    "\n",
    "\n",
    "USER root\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update \\\n",
    "    && apt-get install -y --no-install-recommends \\\n",
    "        build-essential \\\n",
    "        default-libmysqlclient-dev \\\n",
    "        libpq-dev \\\n",
    "    && apt-get autoremove -yqq --purge \\\n",
    "    && apt-get clean \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "USER airflow\n",
    "\n",
    "# Set environment variables explicitly to avoid SQLAlchemy conflicts\n",
    "ENV AIRFLOW__DATABASE__EXECUTEMANY_MODE=batch \n",
    "\n",
    "WORKDIR /app\n",
    "COPY pyproject.toml /app/\n",
    "RUN pip install --no-cache-dir /app/\n",
    "\n",
    "# Only copy your DAGs and plugins when they change\n",
    "COPY src/orchestrators/airflow/dags /opt/airflow/dags\n",
    "COPY src/orchestrators/airflow/plugins /opt/airflow/plugins\n",
    "\n",
    "# Initialize Airflow\n",
    "WORKDIR /opt/airflow \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src\\infrastructure\\docker\\init-db.sql\n"
     ]
    }
   ],
   "source": [
    "%%writefile src\\infrastructure\\docker\\init-db.sql\n",
    "\n",
    "\n",
    "\n",
    "-- Create kestra database\n",
    "CREATE DATABASE kestra;\n",
    "\n",
    "-- Create database for real-time pipeline\n",
    "CREATE DATABASE real_time_pipeline;\n",
    "\n",
    "-- Grant privileges\n",
    "GRANT ALL PRIVILEGES ON DATABASE kestra TO airflow;\n",
    "GRANT ALL PRIVILEGES ON DATABASE real_time_pipeline TO airflow; \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src\\orchestrators\\airflow\\config\\airflow.cfg\n"
     ]
    }
   ],
   "source": [
    "%%writefile src\\orchestrators\\airflow\\config\\airflow.cfg\n",
    "\n",
    "# This is a minimal Airflow configuration file that overrides some default settings\n",
    "# The full configuration is loaded from airflow.cfg in the Airflow installation directory\n",
    "\n",
    "[core]\n",
    "# The folder where your DAGs are located\n",
    "dags_folder = /opt/airflow/dags\n",
    "\n",
    "# The folder where Airflow should store its database\n",
    "sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres:5432/airflow\n",
    "\n",
    "# The executor class that Airflow should use\n",
    "executor = CeleryExecutor\n",
    "\n",
    "# Whether to load the DAG examples that ship with Airflow\n",
    "load_examples = False\n",
    "\n",
    "# Whether DAGs should be loaded by default\n",
    "dags_are_paused_at_creation = True\n",
    "\n",
    "# The amount of parallelism as a setting to the executor\n",
    "parallelism = 32\n",
    "\n",
    "# The number of task instances allowed to run concurrently\n",
    "dag_concurrency = 16\n",
    "\n",
    "# The maximum number of active DAG runs per DAG\n",
    "max_active_runs_per_dag = 16\n",
    "\n",
    "# Enable XCom pickling for TaskFlow API\n",
    "enable_xcom_pickling = True\n",
    "\n",
    "# Enable AssetWatcher\n",
    "task_defer_method = airflow.triggers.triggerer.DeferralTrigger\n",
    "\n",
    "[webserver]\n",
    "# The base URL of your Airflow web server\n",
    "base_url = http://localhost:8080\n",
    "\n",
    "# The default UI theme\n",
    "default_ui_timezone = UTC\n",
    "\n",
    "# The default number of task instances to show in the UI\n",
    "dag_default_view = graph\n",
    "\n",
    "# Default page size when listing entities in the UI\n",
    "page_size = 100\n",
    "\n",
    "[scheduler]\n",
    "# The number of seconds the scheduler should wait between heartbeats to the database\n",
    "scheduler_heartbeat_sec = 5\n",
    "\n",
    "# The number of seconds to wait between file-parsing loops\n",
    "min_file_process_interval = 30\n",
    "\n",
    "# The number of threads to use when processing the dags\n",
    "parsing_processes = 2\n",
    "\n",
    "[celery]\n",
    "# The Celery broker URL\n",
    "broker_url = redis://redis:6379/0\n",
    "\n",
    "# The Celery result backend\n",
    "result_backend = db+postgresql://airflow:airflow@postgres:5432/airflow\n",
    "\n",
    "# The default queue that tasks get assigned to\n",
    "default_queue = default\n",
    "\n",
    "[assetwatcher]\n",
    "# Enable AssetWatcher\n",
    "enable = True \n",
    "\n",
    "[dag_processor]\n",
    "# Use GitDagBundle for versioned DAG source code - commented out as it causes parsing issues\n",
    "# dag_bundle_config_list = [{\n",
    "#   \"name\": \"repo_dags\",\n",
    "#   \"classpath\": \"airflow.providers.git.bundles.git.GitDagBundle\",\n",
    "#   \"kwargs\": {\n",
    "#     \"repo_url\": \"https://github.com/your-org/data_engineering_realtime_pipeline.git\",\n",
    "#     \"tracking_ref\": \"main\",\n",
    "#     \"git_conn_id\": \"git_default\"\n",
    "#   }\n",
    "# }]\n",
    "\n",
    "[database]\n",
    "# Force a supported executemany_mode for this version of SQLAlchemy/psycopg2\n",
    "executemany_mode = batch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src\\orchestrators\\airflow\\dags\\gtfs_data_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src\\orchestrators\\airflow\\dags\\gtfs_data_pipeline.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "GTFS Realtime Data Pipeline DAG \n",
    "\n",
    "This DAG fetches GTFS-RT data from the MTA Bus Time API, processes it,\n",
    "and loads it into the storage backend of choice: S3, BigQuery, Azure Blob, or DuckDB.\n",
    "It also demonstrates SQL operations by loading data into PostgreSQL.\n",
    "\n",
    "The DAG demonstrates the Airflow TaskFlow API (Python functions as tasks)\n",
    "and parameterization for different cloud environments.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "from airflow.decorators import dag, task\n",
    "from airflow.models import Variable, Connection\n",
    "from airflow.operators.python import get_current_context\n",
    "from airflow.utils.dates import days_ago\n",
    "from airflow.providers.postgres.operators.postgres import PostgresOperator\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "\n",
    "# Add the ingestion module to Python path\n",
    "# Adjust this path based on your deployment\n",
    "from ingestion.fetch_gtfs import GTFSFetcher\n",
    "\n",
    "# Default settings applied to all tasks\n",
    "default_args = {\n",
    "    'owner': 'data-engineering',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'execution_timeout': timedelta(minutes=10),\n",
    "}\n",
    "\n",
    "# Configurable parameters with defaults\n",
    "# These can be overridden by setting Airflow Variables\n",
    "CLOUD_PROVIDER = Variable.get(\"CLOUD_PROVIDER\", default_var=\"local\")  # aws, gcp, azure, or local\n",
    "STORAGE_TYPE = Variable.get(\"STORAGE_TYPE\", default_var=\"duckdb\")  # s3, gcs, azure_blob, bigquery, duckdb\n",
    "API_URL = Variable.get(\"GTFS_API_URL\", default_var=\"https://gtfsrt.prod.obanyc.com/vehiclePositions\")\n",
    "OUTPUT_FORMAT = Variable.get(\"OUTPUT_FORMAT\", default_var=\"json\")\n",
    "USE_SQL_DB = Variable.get(\"USE_SQL_DB\", default_var=\"true\").lower() == \"true\"  # Whether to also load data into PostgreSQL\n",
    "\n",
    "# Cloud-specific settings with defaults\n",
    "if CLOUD_PROVIDER == \"aws\":\n",
    "    S3_BUCKET = Variable.get(\"S3_BUCKET\", default_var=\"gtfs-data\")\n",
    "    S3_PREFIX = Variable.get(\"S3_PREFIX\", default_var=\"vehicle_positions\")\n",
    "elif CLOUD_PROVIDER == \"gcp\":\n",
    "    GCS_BUCKET = Variable.get(\"GCS_BUCKET\", default_var=\"gtfs-data\")\n",
    "    GCS_PREFIX = Variable.get(\"GCS_PREFIX\", default_var=\"vehicle_positions\")\n",
    "    BQ_DATASET = Variable.get(\"BQ_DATASET\", default_var=\"gtfs_data\")\n",
    "    BQ_TABLE = Variable.get(\"BQ_TABLE\", default_var=\"vehicle_positions\")\n",
    "elif CLOUD_PROVIDER == \"azure\":\n",
    "    AZURE_CONTAINER = Variable.get(\"AZURE_CONTAINER\", default_var=\"gtfs-data\")\n",
    "    AZURE_PREFIX = Variable.get(\"AZURE_PREFIX\", default_var=\"vehicle_positions\")\n",
    "else:  # local\n",
    "    DUCKDB_PATH = Variable.get(\"DUCKDB_PATH\", default_var=\"/tmp/gtfs.duckdb\")\n",
    "    DUCKDB_TABLE = Variable.get(\"DUCKDB_TABLE\", default_var=\"vehicle_positions\")\n",
    "\n",
    "# Define an asset for asset-driven scheduling\n",
    "from airflow.sdk import Asset, AssetWatcher\n",
    "from airflow.providers.standard.triggers.file import FileSensorTrigger\n",
    "\n",
    "# Create a file sensor trigger for the GTFS asset\n",
    "gtfs_file_trigger = FileSensorTrigger(filepath=\"/data/gtfs/new_data.flag\")\n",
    "gtfs_asset = Asset(\n",
    "    \"gtfs_data_asset\", \n",
    "    watchers=[AssetWatcher(name=\"gtfs_data_watcher\", trigger=gtfs_file_trigger)]\n",
    ")\n",
    "\n",
    "@dag(\n",
    "    default_args=default_args,\n",
    "    schedule=[gtfs_asset],  # asset-driven scheduling\n",
    "    start_date=days_ago(1),\n",
    "    catchup=False,\n",
    "    max_active_runs=1,\n",
    "    tags=['gtfs', 'realtime', 'sql', CLOUD_PROVIDER],\n",
    "    doc_md=__doc__\n",
    ")\n",
    "def gtfs_data_pipeline():\n",
    "    \"\"\"\n",
    "    ### GTFS-RT Data Pipeline\n",
    "\n",
    "    This DAG demonstrates how to fetch and process GTFS-RT data with Airflow,\n",
    "    using different cloud providers and storage backends.\n",
    "\n",
    "    #### Environment configuration\n",
    "    * Cloud Provider: {cloud_provider}\n",
    "    * Storage Type: {storage_type}\n",
    "    * Data Format: {format}\n",
    "    * Also Load to SQL DB: {use_sql_db}\n",
    "    * Schedule: Asset-driven (file trigger)\n",
    "    \"\"\".format(\n",
    "        cloud_provider=CLOUD_PROVIDER,\n",
    "        storage_type=STORAGE_TYPE,\n",
    "        format=OUTPUT_FORMAT,\n",
    "        use_sql_db=USE_SQL_DB\n",
    "    )\n",
    "\n",
    "    @task()\n",
    "    def fetch_gtfs():\n",
    "        \"\"\"Fetch GTFS-RT data from the configured API\"\"\"\n",
    "        # Get API key from connection if configured\n",
    "        try:\n",
    "            conn = Connection.get_connection_from_secrets(\"gtfs_api\")\n",
    "            api_key = conn.password if conn else os.getenv(\"MTA_API_KEY\")\n",
    "        except:\n",
    "            api_key = os.getenv(\"MTA_API_KEY\")\n",
    "\n",
    "        # Initialize fetcher\n",
    "        fetcher = GTFSFetcher(api_url=API_URL, api_key=api_key)\n",
    "\n",
    "        # Get the data\n",
    "        logging.info(f\"Fetching GTFS data from {API_URL}\")\n",
    "        try:\n",
    "            data = fetcher.fetch_and_parse()\n",
    "            logging.info(f\"Successfully fetched {len(data)} GTFS entities\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching GTFS data: {e}\")\n",
    "            raise\n",
    "\n",
    "    @task()\n",
    "    def process_data(data):\n",
    "        \"\"\"Process the GTFS data before storing\"\"\"\n",
    "        # Add processing timestamp\n",
    "        processed_data = []\n",
    "        processing_time = datetime.now().isoformat()\n",
    "\n",
    "        for entity in data:\n",
    "            # Add processing metadata\n",
    "            entity['_processing_time'] = processing_time\n",
    "            processed_data.append(entity)\n",
    "\n",
    "        logging.info(f\"Processed {len(processed_data)} GTFS entities\")\n",
    "        return processed_data\n",
    "\n",
    "    @task()\n",
    "    def transform_for_sql(data):\n",
    "        \"\"\"Transform data into a format suitable for SQL insertion\"\"\"\n",
    "        if not data:\n",
    "            return []\n",
    "            \n",
    "        sql_ready_data = []\n",
    "        for entity in data:\n",
    "            if 'vehicle' in entity and 'position' in entity['vehicle']:\n",
    "                try:\n",
    "                    vehicle_id = entity.get('id', '') or entity['vehicle'].get('vehicle', {}).get('id', 'unknown')\n",
    "                    position = entity['vehicle']['position']\n",
    "                    timestamp = entity['vehicle'].get('timestamp', '')\n",
    "                    \n",
    "                    record = (\n",
    "                        vehicle_id,\n",
    "                        position.get('latitude', 0),\n",
    "                        position.get('longitude', 0),\n",
    "                        position.get('bearing', 0),\n",
    "                        position.get('speed', 0),\n",
    "                        timestamp,\n",
    "                        entity.get('_processing_time', '')\n",
    "                    )\n",
    "                    sql_ready_data.append(record)\n",
    "                except (KeyError, TypeError) as e:\n",
    "                    logging.warning(f\"Could not extract position data from entity: {e}\")\n",
    "        \n",
    "        logging.info(f\"Transformed {len(sql_ready_data)} entities for SQL insertion\")\n",
    "        # Return as a list of tuples for SQL insertion\n",
    "        return sql_ready_data\n",
    "\n",
    "    @task()\n",
    "    def prepare_sql_values(sql_data):\n",
    "        \"\"\"Convert data to SQL VALUES format for PostgresOperator\"\"\"\n",
    "        if not sql_data:\n",
    "            return \"''\"  # Empty string if no data\n",
    "            \n",
    "        # Convert list of tuples to SQL VALUES syntax\n",
    "        values_strings = []\n",
    "        for record in sql_data:\n",
    "            values_str = f\"('{record[0]}', {record[1]}, {record[2]}, {record[3]}, {record[4]}, '{record[5]}', '{record[6]}')\"\n",
    "            values_strings.append(values_str)\n",
    "            \n",
    "        return \", \".join(values_strings)\n",
    "\n",
    "    @task()\n",
    "    def store_data(data):\n",
    "        \"\"\"Store the data in the configured backend\"\"\"\n",
    "        if not data:\n",
    "            logging.warning(\"No data to store\")\n",
    "            return {\"status\": \"warning\", \"message\": \"No data to store\"}\n",
    "\n",
    "        # Get the fetcher for storage methods\n",
    "        try:\n",
    "            conn = Connection.get_connection_from_secrets(\"gtfs_api\")\n",
    "            api_key = conn.password if conn else os.getenv(\"MTA_API_KEY\")\n",
    "        except:\n",
    "            api_key = os.getenv(\"MTA_API_KEY\")\n",
    "\n",
    "        fetcher = GTFSFetcher(api_url=API_URL, api_key=api_key)\n",
    "\n",
    "        # Store based on the configured backend\n",
    "        try:\n",
    "            if CLOUD_PROVIDER == \"aws\":\n",
    "                location = fetcher.save_to_s3(\n",
    "                    data, \n",
    "                    bucket=S3_BUCKET, \n",
    "                    prefix=S3_PREFIX, \n",
    "                    fmt=OUTPUT_FORMAT\n",
    "                )\n",
    "                logging.info(f\"Data saved to S3: {location}\")\n",
    "                return {\"status\": \"success\", \"location\": location}\n",
    "\n",
    "            elif CLOUD_PROVIDER == \"gcp\":\n",
    "                if STORAGE_TYPE == \"bigquery\":\n",
    "                    rows = fetcher.save_to_bigquery(data, BQ_DATASET, BQ_TABLE)\n",
    "                    logging.info(f\"Data saved to BigQuery: {rows} rows\")\n",
    "                    return {\"status\": \"success\", \"rows\": rows}\n",
    "                else:\n",
    "                    location = fetcher.save_to_gcs(\n",
    "                        data, \n",
    "                        bucket=GCS_BUCKET, \n",
    "                        prefix=GCS_PREFIX, \n",
    "                        fmt=OUTPUT_FORMAT\n",
    "                    )\n",
    "                    logging.info(f\"Data saved to GCS: {location}\")\n",
    "                    return {\"status\": \"success\", \"location\": location}\n",
    "\n",
    "            elif CLOUD_PROVIDER == \"azure\":\n",
    "                # Azure implementation would go here\n",
    "                # This would use the Azure blob storage client\n",
    "                logging.info(\"Azure storage not yet implemented\")\n",
    "                return {\"status\": \"not_implemented\", \"message\": \"Azure storage not yet implemented\"}\n",
    "\n",
    "            else:  # local/duckdb\n",
    "                rows = fetcher.save_to_duckdb(data, table=DUCKDB_TABLE, db_path=DUCKDB_PATH)\n",
    "                logging.info(f\"Data saved to DuckDB: {DUCKDB_PATH}, table: {DUCKDB_TABLE}, {rows} rows\")\n",
    "                return {\"status\": \"success\", \"rows\": rows, \"database\": DUCKDB_PATH}\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error storing data: {e}\")\n",
    "            raise\n",
    "\n",
    "    # Create PostgreSQL tables\n",
    "    create_pg_table = PostgresOperator(\n",
    "        task_id=\"create_gtfs_table\",\n",
    "        postgres_conn_id=\"postgres_default\",\n",
    "        sql=\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS public.gtfs_vehicle_positions (\n",
    "            vehicle_id TEXT,\n",
    "            latitude DOUBLE PRECISION,\n",
    "            longitude DOUBLE PRECISION,\n",
    "            bearing DOUBLE PRECISION,\n",
    "            speed DOUBLE PRECISION,\n",
    "            timestamp TIMESTAMP,\n",
    "            processing_time TIMESTAMP,\n",
    "            PRIMARY KEY (vehicle_id, processing_time)\n",
    "        );\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Insert task with dynamic SQL\n",
    "    @task()\n",
    "    def insert_to_postgres(values):\n",
    "        \"\"\"Insert the values into PostgreSQL using PostgresOperator\"\"\"\n",
    "        if not values or values == \"''\":\n",
    "            logging.warning(\"No values to insert into PostgreSQL\")\n",
    "            return {\"rows_inserted\": 0}\n",
    "            \n",
    "        pg_insert = PostgresOperator(\n",
    "            task_id=\"insert_gtfs_data\",\n",
    "            postgres_conn_id=\"postgres_default\",\n",
    "            sql=f\"\"\"\n",
    "            INSERT INTO public.gtfs_vehicle_positions\n",
    "            (vehicle_id, latitude, longitude, bearing, speed, timestamp, processing_time)\n",
    "            VALUES {values}\n",
    "            ON CONFLICT (vehicle_id, processing_time) \n",
    "            DO UPDATE SET\n",
    "                latitude = EXCLUDED.latitude,\n",
    "                longitude = EXCLUDED.longitude,\n",
    "                bearing = EXCLUDED.bearing,\n",
    "                speed = EXCLUDED.speed;\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        pg_insert.execute(context={})\n",
    "        return {\"rows_inserted\": values.count('),') + 1 if values else 0}\n",
    "    \n",
    "    # Task to clean up the flag file that triggered this DAG\n",
    "    @task()\n",
    "    def cleanup():\n",
    "        \"\"\"Clean up the flag file that triggered this DAG\"\"\"\n",
    "        try:\n",
    "            flag_file = \"/data/gtfs/new_data.flag\"\n",
    "            if os.path.exists(flag_file):\n",
    "                os.remove(flag_file)\n",
    "                logging.info(f\"Removed flag file: {flag_file}\")\n",
    "            else:\n",
    "                logging.warning(f\"Flag file not found: {flag_file}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error removing flag file: {e}\")\n",
    "    \n",
    "    # Define SQL branch based on configuration\n",
    "    sql_branch = EmptyOperator(task_id=\"skip_sql_branch\") if not USE_SQL_DB else EmptyOperator(task_id=\"use_sql_branch\")\n",
    "        \n",
    "    # Define the task dependencies\n",
    "    raw_data = fetch_gtfs()\n",
    "    processed_data = process_data(raw_data)\n",
    "    storage_result = store_data(processed_data)\n",
    "    \n",
    "    # SQL branch\n",
    "    if USE_SQL_DB:\n",
    "        sql_data = transform_for_sql(processed_data)\n",
    "        sql_values = prepare_sql_values(sql_data)\n",
    "        create_pg_table >> insert_to_postgres(sql_values) >> cleanup()\n",
    "    \n",
    "    # Main flow\n",
    "    raw_data >> processed_data >> storage_result\n",
    "    \n",
    "    # Return the DAG result\n",
    "    return {\"result\": storage_result}\n",
    "\n",
    "# Instantiate the DAG\n",
    "gtfs_pipeline = gtfs_data_pipeline() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src\\orchestrators\\airflow\\dags\\nba_ingest_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src\\orchestrators\\airflow\\dags\\nba_ingest_pipeline.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "NBA Data Ingestion Pipeline DAG with AssetWatcher\n",
    "\n",
    "This DAG fetches NBA game data from an API, processes it, and loads it into PostgreSQL.\n",
    "It utilizes Airflow 3.0's AssetWatcher for event-driven runs and explicit SQL operations.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "from airflow.decorators import dag, task\n",
    "from airflow.models import Variable\n",
    "from airflow.models.connection import Connection\n",
    "from airflow.providers.postgres.hooks.postgres import PostgresHook\n",
    "from airflow.providers.postgres.operators.postgres import PostgresOperator\n",
    "from airflow.sdk import Asset, AssetWatcher\n",
    "from airflow.providers.standard.triggers.file import FileSensorTrigger\n",
    "from ingestion.fetch_gtfs import GTFSFetcher\n",
    "\n",
    "# Default settings applied to all tasks\n",
    "default_args = {\n",
    "    'owner': 'data-engineering',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'execution_timeout': timedelta(minutes=10),\n",
    "}\n",
    "\n",
    "# Define an asset that watches for new NBA data files in MinIO\n",
    "nba_file_sensor_trigger = FileSensorTrigger(filepath=\"/data/nba/new_data.flag\")\n",
    "nba_data_asset = Asset(\n",
    "    \"nba_data_asset\", \n",
    "    watchers=[AssetWatcher(name=\"nba_data_watcher\", trigger=nba_file_sensor_trigger)]\n",
    ")\n",
    "\n",
    "# Define the DAG using Airflow 3.0's @dag decorator and AssetWatcher\n",
    "@dag(\n",
    "    default_args=default_args,\n",
    "    schedule=[nba_data_asset],  # Schedule based on asset availability\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=['nba', 'event-driven', 'assetwatcher', 'sql'],\n",
    ")\n",
    "def nba_ingest_pipeline():\n",
    "    \"\"\"\n",
    "    ### NBA Data Ingestion Pipeline with AssetWatcher\n",
    "\n",
    "    This DAG demonstrates how to use Airflow 3.0's AssetWatcher for event-driven processing.\n",
    "    It watches for a flag file indicating new NBA data is available, then processes\n",
    "    and loads the data into PostgreSQL using explicit SQL operations.\n",
    "\n",
    "    #### Triggers:\n",
    "    - File sensor watching for \"/data/nba/new_data.flag\"\n",
    "    \"\"\"\n",
    "\n",
    "    @task()\n",
    "    def fetch_nba_games():\n",
    "        \"\"\"Fetch NBA game data from the API\"\"\"\n",
    "        # Get API key from Airflow connection or environment variable\n",
    "        try:\n",
    "            conn = Connection.get_connection_from_secrets(\"nba_api\")\n",
    "            api_key = conn.password\n",
    "            api_url = conn.host\n",
    "        except:\n",
    "            api_key = os.getenv(\"NBA_API_KEY\")\n",
    "            api_url = \"https://api.example.com/nba/games\"\n",
    "\n",
    "        # Get current date in format YYYY-MM-DD\n",
    "        today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # Make API request\n",
    "        headers = {\"API-Key\": api_key} if api_key else {}\n",
    "        params = {\"date\": today}\n",
    "\n",
    "        logging.info(f\"Fetching NBA games data from {api_url} for date {today}\")\n",
    "\n",
    "        # For demo purposes, using sample data if API call fails\n",
    "        try:\n",
    "            response = requests.get(api_url, headers=headers, params=params, timeout=10)\n",
    "            data = response.json().get(\"games\", [])\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to fetch real NBA data: {e}. Using sample data instead.\")\n",
    "            # Sample data for demonstration\n",
    "            data = [\n",
    "                {\n",
    "                    \"id\": \"20240101-LAL-GSW\",\n",
    "                    \"date\": today,\n",
    "                    \"home_team\": \"LAL\",\n",
    "                    \"away_team\": \"GSW\",\n",
    "                    \"score_home\": 120,\n",
    "                    \"score_away\": 115,\n",
    "                },\n",
    "                {\n",
    "                    \"id\": \"20240101-BOS-MIA\",\n",
    "                    \"date\": today,\n",
    "                    \"home_team\": \"BOS\",\n",
    "                    \"away_team\": \"MIA\",\n",
    "                    \"score_home\": 105,\n",
    "                    \"score_away\": 98,\n",
    "                }\n",
    "            ]\n",
    "\n",
    "        logging.info(f\"Retrieved {len(data)} NBA games\")\n",
    "        return data\n",
    "\n",
    "    @task()\n",
    "    def process_games(games_data):\n",
    "        \"\"\"Process the NBA games data\"\"\"\n",
    "        # Add processing metadata\n",
    "        processed_data = []\n",
    "        processing_time = datetime.now().isoformat()\n",
    "\n",
    "        for game in games_data:\n",
    "            # Add processing timestamp\n",
    "            game['processing_time'] = processing_time\n",
    "            processed_data.append(game)\n",
    "\n",
    "        logging.info(f\"Processed {len(processed_data)} NBA games\")\n",
    "        return processed_data\n",
    "    \n",
    "    @task()\n",
    "    def prepare_sql_values(games):\n",
    "        \"\"\"Prepare SQL VALUES for insertion\"\"\"\n",
    "        if not games:\n",
    "            return \"''\"\n",
    "            \n",
    "        values_strings = []\n",
    "        for game in games:\n",
    "            # Format values for SQL INSERT\n",
    "            values_str = f\"('{game['id']}', '{game['date']}', '{game['home_team']}', '{game['away_team']}', {game['score_home']}, {game['score_away']}, CURRENT_TIMESTAMP)\"\n",
    "            values_strings.append(values_str)\n",
    "            \n",
    "        return \", \".join(values_strings)\n",
    "\n",
    "    # Create NBA schema and tables if they don't exist\n",
    "    create_schema = PostgresOperator(\n",
    "        task_id=\"create_nba_schema\",\n",
    "        postgres_conn_id=\"postgres_default\",\n",
    "        sql=\"\"\"\n",
    "        CREATE SCHEMA IF NOT EXISTS nba;\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    create_table = PostgresOperator(\n",
    "        task_id=\"create_nba_games_table\",\n",
    "        postgres_conn_id=\"postgres_default\",\n",
    "        sql=\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS nba.games (\n",
    "            game_id TEXT PRIMARY KEY,\n",
    "            date DATE,\n",
    "            home_team TEXT,\n",
    "            away_team TEXT,\n",
    "            score_home INTEGER,\n",
    "            score_away INTEGER,\n",
    "            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # PostgresOperator task that takes values from previous task\n",
    "    @task()\n",
    "    def insert_games_to_db(values):\n",
    "        \"\"\"Insert games data into database using PostgresOperator\"\"\"\n",
    "        if not values or values == \"''\":\n",
    "            logging.warning(\"No NBA games data to save\")\n",
    "            return {\"status\": \"warning\", \"message\": \"No data to save\"}\n",
    "            \n",
    "        insert_games = PostgresOperator(\n",
    "            task_id=\"insert_nba_games\",\n",
    "            postgres_conn_id=\"postgres_default\",\n",
    "            sql=f\"\"\"\n",
    "            INSERT INTO nba.games \n",
    "            (game_id, date, home_team, away_team, score_home, score_away, updated_at)\n",
    "            VALUES {values}\n",
    "            ON CONFLICT (game_id) \n",
    "            DO UPDATE SET \n",
    "                score_home = EXCLUDED.score_home,\n",
    "                score_away = EXCLUDED.score_away,\n",
    "                updated_at = CURRENT_TIMESTAMP;\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        insert_games.execute(context={})\n",
    "        return {\"status\": \"success\", \"count\": values.count('),') + 1 if values else 0}\n",
    "    \n",
    "    # Create summary view for analytics\n",
    "    create_summary_view = PostgresOperator(\n",
    "        task_id=\"create_nba_summary_view\",\n",
    "        postgres_conn_id=\"postgres_default\",\n",
    "        sql=\"\"\"\n",
    "        CREATE OR REPLACE VIEW nba.games_summary AS\n",
    "        SELECT\n",
    "            date,\n",
    "            COUNT(*) AS game_count,\n",
    "            AVG(score_home + score_away) AS avg_total_score,\n",
    "            MAX(score_home) AS max_home_score,\n",
    "            MAX(score_away) AS max_away_score,\n",
    "            SUM(CASE WHEN score_home > score_away THEN 1 ELSE 0 END) AS home_wins,\n",
    "            SUM(CASE WHEN score_home < score_away THEN 1 ELSE 0 END) AS away_wins\n",
    "        FROM nba.games\n",
    "        GROUP BY date\n",
    "        ORDER BY date DESC;\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Run analytics query - this will show up in UI\n",
    "    run_analytics = PostgresOperator(\n",
    "        task_id=\"run_nba_analytics\",\n",
    "        postgres_conn_id=\"postgres_default\",\n",
    "        sql=\"\"\"\n",
    "        -- Calculate team performance metrics\n",
    "        SELECT \n",
    "            home_team AS team,\n",
    "            COUNT(*) AS games_played,\n",
    "            SUM(CASE WHEN score_home > score_away THEN 1 ELSE 0 END) AS wins,\n",
    "            SUM(CASE WHEN score_home < score_away THEN 1 ELSE 0 END) AS losses,\n",
    "            AVG(score_home) AS avg_points_scored\n",
    "        FROM nba.games\n",
    "        GROUP BY home_team\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        SELECT \n",
    "            away_team AS team,\n",
    "            COUNT(*) AS games_played,\n",
    "            SUM(CASE WHEN score_away > score_home THEN 1 ELSE 0 END) AS wins,\n",
    "            SUM(CASE WHEN score_away < score_home THEN 1 ELSE 0 END) AS losses,\n",
    "            AVG(score_away) AS avg_points_scored\n",
    "        FROM nba.games\n",
    "        GROUP BY away_team\n",
    "        ORDER BY wins DESC, team;\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    @task()\n",
    "    def cleanup():\n",
    "        \"\"\"Clean up temporary files after processing\"\"\"\n",
    "        # Remove the flag file that triggered this DAG\n",
    "        try:\n",
    "            flag_file = \"/data/nba/new_data.flag\"\n",
    "            if os.path.exists(flag_file):\n",
    "                os.remove(flag_file)\n",
    "                logging.info(f\"Removed flag file: {flag_file}\")\n",
    "            else:\n",
    "                logging.warning(f\"Flag file not found: {flag_file}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error removing flag file: {e}\")\n",
    "            \n",
    "        return {\"status\": \"success\"}\n",
    "\n",
    "    # Define task dependencies\n",
    "    nba_games = fetch_nba_games()\n",
    "    processed_games = process_games(nba_games)\n",
    "    sql_values = prepare_sql_values(processed_games)\n",
    "    \n",
    "    # Database operations\n",
    "    create_schema >> create_table\n",
    "    \n",
    "    # Execute insert and analytics\n",
    "    insert_result = insert_games_to_db(sql_values)\n",
    "    create_table >> insert_result >> create_summary_view >> run_analytics\n",
    "    \n",
    "    # Cleanup at the end\n",
    "    cleanup_result = cleanup()\n",
    "    run_analytics >> cleanup_result\n",
    "\n",
    "    # Set up main flow\n",
    "    nba_games >> processed_games >> sql_values >> insert_result\n",
    "\n",
    "# Instantiate the DAG\n",
    "nba_pipeline = nba_ingest_pipeline() \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src\\orchestrators\\airflow\\dags\\weather_kafka_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src\\orchestrators\\airflow\\dags\\weather_kafka_pipeline.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Weather Data Kafka Pipeline DAG with AssetWatcher\n",
    "\n",
    "This DAG processes weather data from Kafka, transforms it, and loads it into PostgreSQL.\n",
    "It demonstrates Airflow 3.0's AssetWatcher for Kafka-driven events and explicit SQL operations.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "from airflow.decorators import dag, task\n",
    "from airflow.models import Variable\n",
    "from airflow.providers.postgres.hooks.postgres import PostgresHook\n",
    "from airflow.providers.postgres.operators.postgres import PostgresOperator\n",
    "from airflow.sdk import Asset, AssetWatcher\n",
    "from airflow.providers.standard.triggers.file import FileSensorTrigger\n",
    "from airflow.providers.kafka.hooks.kafka import KafkaHook\n",
    "from ingestion.fetch_gtfs import GTFSFetcher\n",
    "\n",
    "# Default settings applied to all tasks\n",
    "default_args = {\n",
    "    'owner': 'data-engineering',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'execution_timeout': timedelta(minutes=10),\n",
    "}\n",
    "\n",
    "# Define an asset that watches for a file indicating new Kafka messages\n",
    "weather_file_trigger = FileSensorTrigger(filepath=\"/data/weather/new_data.flag\")\n",
    "weather_data_asset = Asset(\n",
    "    \"weather_data_asset\", \n",
    "    watchers=[AssetWatcher(name=\"weather_data_watcher\", trigger=weather_file_trigger)]\n",
    ")\n",
    "\n",
    "@dag(\n",
    "    default_args=default_args,\n",
    "    schedule=[weather_data_asset],  # Event-driven scheduling\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=['weather', 'kafka', 'event-driven', 'assetwatcher', 'sql'],\n",
    ")\n",
    "def weather_kafka_pipeline():\n",
    "    \"\"\"\n",
    "    ### Weather Data Kafka Pipeline with AssetWatcher\n",
    "\n",
    "    This DAG demonstrates how to process weather data from Kafka using\n",
    "    Airflow 3.0's AssetWatcher for event-driven runs. It consumes messages\n",
    "    from a Kafka topic and stores them in PostgreSQL using explicit SQL operations.\n",
    "\n",
    "    #### Triggers:\n",
    "    - File sensor watching for \"/data/weather/new_data.flag\"\n",
    "    \"\"\"\n",
    "\n",
    "    @task()\n",
    "    def consume_kafka_messages():\n",
    "        \"\"\"Consume weather data from Kafka\"\"\"\n",
    "        topic = Variable.get(\"WEATHER_KAFKA_TOPIC\", default_var=\"weather-updates\")\n",
    "        max_messages = int(Variable.get(\"WEATHER_MAX_MESSAGES\", default_var=\"100\"))\n",
    "\n",
    "        kafka_hook = KafkaHook(kafka_conn_id=\"kafka_default\")\n",
    "        consumer = kafka_hook.get_consumer(\n",
    "            topics=[topic],\n",
    "            group_id=\"weather-pipeline-group\",\n",
    "            auto_offset_reset=\"earliest\",\n",
    "            enable_auto_commit=False,\n",
    "            consumer_timeout_ms=10000  # 10 seconds timeout\n",
    "        )\n",
    "\n",
    "        messages = []\n",
    "        message_count = 0\n",
    "\n",
    "        logging.info(f\"Starting to consume messages from Kafka topic: {topic}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Use poll to get better control over consumption\n",
    "        try:\n",
    "            while message_count < max_messages:\n",
    "                poll_result = consumer.poll(timeout_ms=5000, max_records=max_messages)\n",
    "                if not poll_result:\n",
    "                    break\n",
    "\n",
    "                # Process all partitions and messages\n",
    "                for tp, records in poll_result.items():\n",
    "                    for record in records:\n",
    "                        try:\n",
    "                            message = json.loads(record.value.decode('utf-8'))\n",
    "                            message['_metadata'] = {\n",
    "                                'topic': record.topic,\n",
    "                                'partition': record.partition,\n",
    "                                'offset': record.offset,\n",
    "                                'timestamp': record.timestamp\n",
    "                            }\n",
    "                            messages.append(message)\n",
    "                            message_count += 1\n",
    "                        except json.JSONDecodeError:\n",
    "                            logging.warning(f\"Skipping non-JSON message: {record.value}\")\n",
    "\n",
    "                # Commit offsets after processing\n",
    "                consumer.commit()\n",
    "\n",
    "            logging.info(f\"Consumed {message_count} messages in {time.time() - start_time:.2f} seconds\")\n",
    "        finally:\n",
    "            consumer.close()\n",
    "\n",
    "        return messages\n",
    "\n",
    "    @task()\n",
    "    def process_weather_data(messages):\n",
    "        \"\"\"Process and transform the weather data\"\"\"\n",
    "        if not messages:\n",
    "            logging.warning(\"No weather messages to process\")\n",
    "            return []\n",
    "\n",
    "        processed_data = []\n",
    "        processing_time = datetime.now().isoformat()\n",
    "\n",
    "        for message in messages:\n",
    "            try:\n",
    "                # Extract weather observation data\n",
    "                observation = {\n",
    "                    'location': message.get('location', 'unknown'),\n",
    "                    'latitude': message.get('lat'),\n",
    "                    'longitude': message.get('lon'),\n",
    "                    'obs_time': message.get('observation_time'),\n",
    "                    'temperature': message.get('temp_c'),\n",
    "                    'humidity': message.get('humidity'),\n",
    "                    'pressure': message.get('pressure_mb'),\n",
    "                    'wind_speed': message.get('wind_kph'),\n",
    "                    'wind_direction': message.get('wind_dir'),\n",
    "                    'conditions': message.get('condition', {}).get('text'),\n",
    "                    '_processing_time': processing_time\n",
    "                }\n",
    "                processed_data.append(observation)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing weather message: {e}\")\n",
    "\n",
    "        logging.info(f\"Processed {len(processed_data)} weather observations\")\n",
    "        return processed_data\n",
    "    \n",
    "    @task()\n",
    "    def prepare_sql_values(observations):\n",
    "        \"\"\"Prepare SQL VALUES for insertion\"\"\"\n",
    "        if not observations:\n",
    "            return \"''\"\n",
    "            \n",
    "        values_strings = []\n",
    "        for obs in observations:\n",
    "            # Format values for SQL INSERT\n",
    "            values_str = f\"\"\"(\n",
    "                '{obs['location']}', \n",
    "                {obs['latitude'] if obs['latitude'] is not None else 'NULL'}, \n",
    "                {obs['longitude'] if obs['longitude'] is not None else 'NULL'}, \n",
    "                '{obs['obs_time']}', \n",
    "                {obs['temperature'] if obs['temperature'] is not None else 'NULL'}, \n",
    "                {obs['humidity'] if obs['humidity'] is not None else 'NULL'}, \n",
    "                {obs['pressure'] if obs['pressure'] is not None else 'NULL'}, \n",
    "                {obs['wind_speed'] if obs['wind_speed'] is not None else 'NULL'}, \n",
    "                '{obs['wind_direction']}', \n",
    "                '{obs['conditions'] if obs['conditions'] else ''}'\n",
    "            )\"\"\"\n",
    "            values_strings.append(values_str)\n",
    "            \n",
    "        return \", \".join(values_strings)\n",
    "\n",
    "    # Create schema and tables with PostgresOperator\n",
    "    create_schema = PostgresOperator(\n",
    "        task_id=\"create_weather_schema\",\n",
    "        postgres_conn_id=\"postgres_default\",\n",
    "        sql=\"\"\"\n",
    "        CREATE SCHEMA IF NOT EXISTS weather;\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    create_weather_table = PostgresOperator(\n",
    "        task_id=\"create_weather_table\",\n",
    "        postgres_conn_id=\"postgres_default\",\n",
    "        sql=\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS weather.observations (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            location TEXT NOT NULL,\n",
    "            latitude DOUBLE PRECISION,\n",
    "            longitude DOUBLE PRECISION,\n",
    "            obs_time TIMESTAMP,\n",
    "            temperature DOUBLE PRECISION,\n",
    "            humidity DOUBLE PRECISION,\n",
    "            pressure DOUBLE PRECISION,\n",
    "            wind_speed DOUBLE PRECISION,\n",
    "            wind_direction TEXT,\n",
    "            conditions TEXT,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # PostgresOperator task that takes values from previous task\n",
    "    @task()\n",
    "    def insert_weather_data(values):\n",
    "        \"\"\"Insert weather data using PostgresOperator\"\"\"\n",
    "        if not values or values == \"''\":\n",
    "            logging.warning(\"No weather data to insert\")\n",
    "            return {\"status\": \"warning\", \"message\": \"No data to insert\"}\n",
    "            \n",
    "        insert_data = PostgresOperator(\n",
    "            task_id=\"insert_weather_data\",\n",
    "            postgres_conn_id=\"postgres_default\",\n",
    "            sql=f\"\"\"\n",
    "            INSERT INTO weather.observations\n",
    "            (location, latitude, longitude, obs_time, temperature, humidity,\n",
    "             pressure, wind_speed, wind_direction, conditions)\n",
    "            VALUES {values};\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        insert_data.execute(context={})\n",
    "        return {\"status\": \"success\", \"count\": values.count('),') + 1 if values else 0}\n",
    "    \n",
    "    # Create summary tables and views\n",
    "    create_daily_summary = PostgresOperator(\n",
    "        task_id=\"create_weather_daily_summary\",\n",
    "        postgres_conn_id=\"postgres_default\",\n",
    "        sql=\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS weather.daily_summary (\n",
    "            summary_date DATE PRIMARY KEY,\n",
    "            location_count INTEGER,\n",
    "            avg_temperature DOUBLE PRECISION,\n",
    "            min_temperature DOUBLE PRECISION,\n",
    "            max_temperature DOUBLE PRECISION,\n",
    "            avg_humidity DOUBLE PRECISION,\n",
    "            avg_pressure DOUBLE PRECISION,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Run analytics to summarize weather data\n",
    "    run_analytics = PostgresOperator(\n",
    "        task_id=\"run_weather_analytics\",\n",
    "        postgres_conn_id=\"postgres_default\",\n",
    "        sql=\"\"\"\n",
    "        -- Insert into daily summary table\n",
    "        INSERT INTO weather.daily_summary\n",
    "        (summary_date, location_count, avg_temperature, min_temperature, \n",
    "         max_temperature, avg_humidity, avg_pressure)\n",
    "        SELECT \n",
    "            DATE(obs_time) AS summary_date,\n",
    "            COUNT(DISTINCT location) AS location_count,\n",
    "            AVG(temperature) AS avg_temperature,\n",
    "            MIN(temperature) AS min_temperature,\n",
    "            MAX(temperature) AS max_temperature,\n",
    "            AVG(humidity) AS avg_humidity,\n",
    "            AVG(pressure) AS avg_pressure\n",
    "        FROM weather.observations\n",
    "        WHERE obs_time >= CURRENT_DATE - INTERVAL '1 day'\n",
    "        GROUP BY DATE(obs_time)\n",
    "        ON CONFLICT (summary_date) \n",
    "        DO UPDATE SET\n",
    "            location_count = EXCLUDED.location_count,\n",
    "            avg_temperature = EXCLUDED.avg_temperature,\n",
    "            min_temperature = EXCLUDED.min_temperature,\n",
    "            max_temperature = EXCLUDED.max_temperature,\n",
    "            avg_humidity = EXCLUDED.avg_humidity,\n",
    "            avg_pressure = EXCLUDED.avg_pressure,\n",
    "            created_at = CURRENT_TIMESTAMP;\n",
    "            \n",
    "        -- Calculate location-level statistics\n",
    "        SELECT \n",
    "            location,\n",
    "            COUNT(*) AS observation_count,\n",
    "            AVG(temperature) AS avg_temp,\n",
    "            AVG(humidity) AS avg_humidity,\n",
    "            MIN(temperature) AS min_temp,\n",
    "            MAX(temperature) AS max_temp\n",
    "        FROM weather.observations\n",
    "        WHERE obs_time >= CURRENT_DATE - INTERVAL '7 days'\n",
    "        GROUP BY location\n",
    "        ORDER BY observation_count DESC;\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    @task()\n",
    "    def cleanup():\n",
    "        \"\"\"Clean up temporary files after processing\"\"\"\n",
    "        try:\n",
    "            flag_file = \"/data/weather/new_data.flag\"\n",
    "            if os.path.exists(flag_file):\n",
    "                os.remove(flag_file)\n",
    "                logging.info(f\"Removed flag file: {flag_file}\")\n",
    "            else:\n",
    "                logging.warning(f\"Flag file not found: {flag_file}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error removing flag file: {e}\")\n",
    "        \n",
    "        return {\"status\": \"success\"}\n",
    "\n",
    "    # Define task dependencies\n",
    "    kafka_messages = consume_kafka_messages()\n",
    "    processed_data = process_weather_data(kafka_messages)\n",
    "    sql_values = prepare_sql_values(processed_data)\n",
    "    \n",
    "    # Database tasks\n",
    "    create_schema >> create_weather_table\n",
    "    create_weather_table >> create_daily_summary\n",
    "    \n",
    "    # Insert and analyze\n",
    "    insert_result = insert_weather_data(sql_values)\n",
    "    create_daily_summary >> insert_result >> run_analytics\n",
    "    \n",
    "    # Cleanup\n",
    "    cleanup_result = cleanup()\n",
    "    run_analytics >> cleanup_result\n",
    "    \n",
    "    # Set up main flow\n",
    "    kafka_messages >> processed_data >> sql_values >> insert_result\n",
    "\n",
    "# Instantiate the DAG\n",
    "weather_pipeline = weather_kafka_pipeline() \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src\\orchestrators\\airflow\\dags\\user_metrics_etl.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src\\orchestrators\\airflow\\dags\\user_metrics_etl.py\n",
    "\n",
    "# src/orchestrators/airflow/dags/user_metrics_etl.py\n",
    "from airflow.decorators import dag, task\n",
    "from airflow.providers.postgres.operators.postgres import PostgresOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "@dag(start_date=days_ago(1), schedule_interval=\"@daily\", tags=[\"metrics\", \"sql\"])\n",
    "def user_metrics_etl():\n",
    "    @task()\n",
    "    def extract():\n",
    "        # \n",
    "        return data\n",
    "\n",
    "    @task()\n",
    "    def transform(data):\n",
    "        # \n",
    "        return sql_values\n",
    "\n",
    "    create_table = PostgresOperator(\n",
    "        task_id=\"create_user_metrics_table\",\n",
    "        postgres_conn_id=\"postgres_default\",\n",
    "        sql=\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS user_metrics ();\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    load = PostgresOperator(\n",
    "        task_id=\"load_user_metrics\",\n",
    "        postgres_conn_id=\"postgres_default\",\n",
    "        sql=\"INSERT INTO user_metrics VALUES {{ params.values }};\",\n",
    "        parameters={\"values\": \"{{ ti.xcom_pull('transform') }}\"}\n",
    "    )\n",
    "\n",
    "    data = extract()\n",
    "    vals = transform(data)\n",
    "    create_table >> load\n",
    "\n",
    "user_metrics_etl_dag = user_metrics_etl()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src\\orchestrators\\airflow\\tests\\test_dag_integrity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src\\orchestrators\\airflow\\tests\\test_dag_integrity.py\n",
    "\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Tests for Airflow DAG integrity.\n",
    "\n",
    "This module contains tests to ensure our DAGs load and have a valid structure.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import unittest\n",
    "from pathlib import Path\n",
    "from airflow.models import DagBag\n",
    "\n",
    "# Get DAGs directory\n",
    "DAGS_DIR = Path(__file__).parents[1] / \"dags\"\n",
    "\n",
    "\n",
    "class TestDagIntegrity(unittest.TestCase):\n",
    "    \"\"\"Test DAG integrity.\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        \"\"\"Set up test class.\"\"\"\n",
    "        cls.dagbag = DagBag(dag_folder=str(DAGS_DIR), include_examples=False)\n",
    "\n",
    "    def test_no_import_errors(self):\n",
    "        \"\"\"Test there are no import errors in our DAGs.\"\"\"\n",
    "        import_errors = self.dagbag.import_errors\n",
    "        self.assertEqual(\n",
    "            len(import_errors), 0, f\"DAG import errors: {import_errors}\"\n",
    "        )\n",
    "\n",
    "    def test_all_dags_have_required_fields(self):\n",
    "        \"\"\"Test that all DAGs have the required fields.\"\"\"\n",
    "        for dag_id, dag in self.dagbag.dags.items():\n",
    "            self.assertIsNotNone(dag.default_args.get('owner', None),\n",
    "                                f\"{dag_id}: Missing owner in default_args\")\n",
    "            self.assertIsNotNone(dag.default_args.get('retries', None),\n",
    "                                f\"{dag_id}: Missing retries in default_args\")\n",
    "            self.assertIsNotNone(dag.default_args.get('retry_delay', None),\n",
    "                                f\"{dag_id}: Missing retry_delay in default_args\")\n",
    "\n",
    "    def test_all_dags_have_tags(self):\n",
    "        \"\"\"Test that all DAGs have tags.\"\"\"\n",
    "        for dag_id, dag in self.dagbag.dags.items():\n",
    "            tags = dag.tags\n",
    "            self.assertTrue(\n",
    "                len(tags) > 0, f\"{dag_id}: No tags specified for DAG\"\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
