{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f76972b9",
   "metadata": {},
   "source": [
    "# Real-Time Prediction with MTA Bus Time GTFS-Realtime Feed\n",
    "\n",
    "This notebook demonstrates a real-time data pipeline using the MTA Bus Time GTFS-Realtime feed. The pipeline ingests, processes, stores, queries, and visualizes bus data using Kafka, Flink, Iceberg, Trino, and Superset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecbb45c",
   "metadata": {},
   "source": [
    "## 1. Setup and Install Dependencies\n",
    "\n",
    "This project uses a `pyproject.toml` file for dependency management. Make sure to install the dependencies using `pip` or `uv sync` before running this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e29b86",
   "metadata": {},
   "source": [
    "## 2. Access and Fetch GTFS-Realtime Data\n",
    "\n",
    "Use the MTA GTFS-Realtime API to fetch vehicle positions, trip updates, and alerts. Parse the protobuf data into a readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac8ae691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched 2419 vehicle positions\n",
      "[{'id': 'MTA NYCT_9770', 'vehicle': {'trip': {'tripId': 'MV_B5-Weekday-SDon-042000_M116_X01', 'startDate': '20250501', 'routeId': 'M116', 'directionId': 1}, 'position': {'latitude': 40.79613, 'longitude': -73.93236, 'bearing': 233.86462}, 'timestamp': '1746095839', 'stopId': '405315', 'vehicle': {'id': 'MTA NYCT_9770'}}}, {'id': 'MTA NYCT_8444', 'vehicle': {'trip': {'tripId': 'JA_B5-Weekday-SDon-037000_Q30_402', 'startDate': '20250501', 'routeId': 'Q30', 'directionId': 0}, 'position': {'latitude': 40.74045, 'longitude': -73.78713, 'bearing': 14.370002}, 'timestamp': '1746095838', 'stopId': '501444', 'vehicle': {'id': 'MTA NYCT_8444'}}}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from google.protobuf import json_format\n",
    "from google.transit import gtfs_realtime_pb2\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key from .env file if available\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"MTA_API_KEY\", \"YOUR_MTA_API_KEY\")\n",
    "\n",
    "# Example: Fetch vehicle positions from MTA Bus Time GTFS-Realtime API\n",
    "VEHICLE_POSITIONS_URL = f\"http://gtfsrt.prod.obanyc.com/vehiclePositions?key={API_KEY}\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(VEHICLE_POSITIONS_URL)\n",
    "    if response.status_code == 200:\n",
    "        feed = gtfs_realtime_pb2.FeedMessage()\n",
    "        feed.ParseFromString(response.content)\n",
    "        \n",
    "        # Convert protobuf to JSON\n",
    "        vehicle_positions = [json_format.MessageToDict(entity) for entity in feed.entity]\n",
    "        print(f\"Successfully fetched {len(vehicle_positions)} vehicle positions\")\n",
    "        print(vehicle_positions[:2] if vehicle_positions else \"No data available\")  # Preview first two entries\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        print(\"Note: You may need to set up a valid MTA API key in a .env file\")\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching data: {e}\")\n",
    "    print(\"\\nNOTE: If you're getting module import errors, please run the first cell to install dependencies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9876d8f1",
   "metadata": {},
   "source": [
    "## Setup Docker Compose for Kafka, Flink, and Trino\n",
    "\n",
    "Creating a Docker Compose setup for local development of the pipeline components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "667acb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing docker-compose.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile docker-compose.yml\n",
    "services:\n",
    "  # Zookeeper for Kafka\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:7.4.0\n",
    "    hostname: zookeeper\n",
    "    container_name: zookeeper\n",
    "    ports:\n",
    "      - \"2181:2181\"\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 2181\n",
    "      ZOOKEEPER_TICK_TIME: 2000\n",
    "\n",
    "  # Kafka broker\n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:7.4.0\n",
    "    hostname: kafka\n",
    "    container_name: kafka\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "    ports:\n",
    "      - \"9092:9092\"\n",
    "      - \"29092:29092\"\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092\n",
    "      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n",
    "      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n",
    "\n",
    "  # Kafka Control Center\n",
    "  control-center:\n",
    "    image: confluentinc/cp-enterprise-control-center:7.4.0\n",
    "    hostname: control-center\n",
    "    container_name: control-center\n",
    "    depends_on:\n",
    "      - kafka\n",
    "    ports:\n",
    "      - \"9021:9021\"\n",
    "    environment:\n",
    "      CONTROL_CENTER_BOOTSTRAP_SERVERS: kafka:9092\n",
    "      CONTROL_CENTER_REPLICATION_FACTOR: 1\n",
    "      PORT: 9021\n",
    "\n",
    "  # Flink JobManager\n",
    "  flink-jobmanager:\n",
    "    image: apache/flink:1.18\n",
    "    hostname: jobmanager\n",
    "    container_name: flink-jobmanager\n",
    "    ports:\n",
    "      - \"8081:8081\"\n",
    "    command: jobmanager\n",
    "    environment:\n",
    "      - |  \n",
    "        FLINK_PROPERTIES=\n",
    "        jobmanager.rpc.address: jobmanager        \n",
    "        parallelism.default: 1\n",
    "\n",
    "  # Flink TaskManager\n",
    "  flink-taskmanager:\n",
    "    image: apache/flink:1.18\n",
    "    hostname: taskmanager\n",
    "    container_name: flink-taskmanager\n",
    "    depends_on:\n",
    "      - flink-jobmanager\n",
    "    command: taskmanager\n",
    "    environment:\n",
    "      - |  \n",
    "        FLINK_PROPERTIES=\n",
    "        jobmanager.rpc.address: jobmanager        \n",
    "        taskmanager.numberOfTaskSlots: 2\n",
    "        parallelism.default: 1\n",
    "\n",
    "  # MinIO (S3-compatible storage for Iceberg)\n",
    "  minio:\n",
    "    image: minio/minio\n",
    "    container_name: minio\n",
    "    ports:\n",
    "      - \"9000:9000\"\n",
    "      - \"9001:9001\"\n",
    "    environment:\n",
    "      MINIO_ROOT_USER: minio\n",
    "      MINIO_ROOT_PASSWORD: minio123\n",
    "    command: server /data --console-address \":9001\"\n",
    "    volumes:\n",
    "      - minio-data:/data\n",
    "\n",
    "  # Trino coordinator\n",
    "  trino-coordinator:\n",
    "    image: trinodb/trino:413\n",
    "    container_name: trino-coordinator\n",
    "    ports:\n",
    "      - \"8080:8080\"\n",
    "    volumes:\n",
    "      - ./trino/catalog:/etc/trino/catalog\n",
    "      - ./trino/etc:/etc/trino/etc\n",
    "\n",
    "volumes:\n",
    "  minio-data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3f3667",
   "metadata": {},
   "source": [
    "## 3. Publish Data to Kafka\n",
    "\n",
    "Create Kafka topics and write a Python producer to publish GTFS-Realtime data to Kafka topics such as `vehicle_positions`, `trip_updates`, and `alerts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b2f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Update Kafka broker address based on Docker Compose configuration\n",
    "KAFKA_BROKER = 'localhost:29092'  # External port for local access\n",
    "TOPIC = 'vehicle_positions'\n",
    "\n",
    "def create_kafka_producer():\n",
    "    try:\n",
    "        producer_config = {\n",
    "            'bootstrap.servers': KAFKA_BROKER,\n",
    "            'client.id': 'gtfs-realtime-producer'\n",
    "        }\n",
    "        producer = Producer(producer_config)\n",
    "        print(f\"Successfully connected to Kafka broker at {KAFKA_BROKER}\")\n",
    "        return producer\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating Kafka producer: {e}\")\n",
    "        print(\"\\nNOTE: Make sure Docker Compose is running with Kafka services.\")\n",
    "        return None\n",
    "\n",
    "def delivery_report(err, msg):\n",
    "    if err is not None:\n",
    "        print(f\"Delivery failed: {err}\")\n",
    "    else:\n",
    "        print(f\"Message delivered to {msg.topic()} [{msg.partition()}]\")\n",
    "\n",
    "# Only run if we have vehicle positions data\n",
    "if 'vehicle_positions' in locals() and vehicle_positions:\n",
    "    producer = create_kafka_producer()\n",
    "    if producer:\n",
    "        try:\n",
    "            # Publish vehicle positions to Kafka\n",
    "            print(f\"Publishing {len(vehicle_positions)} records to Kafka topic: {TOPIC}\")\n",
    "            for vp in vehicle_positions:\n",
    "                producer.produce(TOPIC, json.dumps(vp).encode('utf-8'), callback=delivery_report)\n",
    "                producer.poll(0)  # Trigger callbacks\n",
    "            \n",
    "            producer.flush()\n",
    "            print(\"All messages published to Kafka\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error publishing to Kafka: {e}\")\n",
    "else:\n",
    "    print(\"No vehicle position data available to publish to Kafka.\")\n",
    "    print(\"Run the previous cell to fetch GTFS-Realtime data first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7253631",
   "metadata": {},
   "source": [
    "## 4. Stream Processing with Apache Flink\n",
    "\n",
    "Set up a Flink job to consume data from Kafka, process it (e.g., calculate headway variance, detect bus bunching), and write the results to Iceberg tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce529360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Flink job setup for processing GTFS-Realtime data\n",
    "# This would be implemented in a separate Flink application\n",
    "\n",
    "%%writefile flink_job_example.py\n",
    "# This is a sample PyFlink job to process vehicle position data\n",
    "# You would submit this to the Flink cluster we set up in Docker Compose\n",
    "\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import StreamTableEnvironment, EnvironmentSettings\n",
    "\n",
    "def run_flink_job():\n",
    "    # Set up the execution environment\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "    env_settings = EnvironmentSettings.Builder().build()\n",
    "    table_env = StreamTableEnvironment.create(env, environment_settings=env_settings)\n",
    "    \n",
    "    # Define Kafka source\n",
    "    table_env.execute_sql(\"\"\"\n",
    "    CREATE TABLE vehicle_positions (\n",
    "        vehicle_id STRING,\n",
    "        trip_id STRING,\n",
    "        route_id STRING,\n",
    "        timestamp BIGINT,\n",
    "        latitude DOUBLE,\n",
    "        longitude DOUBLE,\n",
    "        speed DOUBLE,\n",
    "        bearing DOUBLE,\n",
    "        event_time AS TO_TIMESTAMP(FROM_UNIXTIME(timestamp / 1000)),\n",
    "        WATERMARK FOR event_time AS event_time - INTERVAL '5' SECONDS\n",
    "    ) WITH (\n",
    "        'connector' = 'kafka',\n",
    "        'topic' = 'vehicle_positions',\n",
    "        'properties.bootstrap.servers' = 'kafka:9092',\n",
    "        'properties.group.id' = 'flink-vehicle-analyzer',\n",
    "        'format' = 'json',\n",
    "        'scan.startup.mode' = 'latest-offset'\n",
    "    )\n",
    "    \"\"\")\n",
    "    \n",
    "    # Define Iceberg sink for processed data\n",
    "    table_env.execute_sql(\"\"\"\n",
    "    CREATE TABLE vehicle_metrics (\n",
    "        route_id STRING,\n",
    "        window_start TIMESTAMP(3),\n",
    "        window_end TIMESTAMP(3),\n",
    "        vehicle_count BIGINT,\n",
    "        avg_speed DOUBLE,\n",
    "        PRIMARY KEY (route_id, window_start) NOT ENFORCED\n",
    "    ) WITH (\n",
    "        'connector' = 'iceberg',\n",
    "        'catalog-name' = 'iceberg_catalog',\n",
    "        'catalog-type' = 'hive',\n",
    "        'warehouse' = 's3a://warehouse/iceberg',\n",
    "        'format-version' = '2'\n",
    "    )\n",
    "    \"\"\")\n",
    "    \n",
    "    # Calculate metrics with a window aggregation\n",
    "    table_env.execute_sql(\"\"\"\n",
    "    INSERT INTO vehicle_metrics\n",
    "    SELECT\n",
    "        route_id,\n",
    "        window_start,\n",
    "        window_end,\n",
    "        COUNT(DISTINCT vehicle_id) AS vehicle_count,\n",
    "        AVG(speed) AS avg_speed\n",
    "    FROM TABLE(\n",
    "        TUMBLE(TABLE vehicle_positions, DESCRIPTOR(event_time), INTERVAL '1' MINUTE)\n",
    "    )\n",
    "    GROUP BY route_id, window_start, window_end\n",
    "    \"\"\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_flink_job()\n",
    "\n",
    "print(\"Sample Flink job code has been generated to flink_job_example.py\")\n",
    "print(\"NOTE: To run this job, you would need to submit it to the Flink cluster\")\n",
    "print(\"using the Flink CLI or REST API.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13d012e",
   "metadata": {},
   "source": [
    "## 5. Store Processed Data in Iceberg\n",
    "\n",
    "Configure Iceberg to store processed data in a time-partitioned format. Define schemas for raw and aggregated data tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50a109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Trino catalog configuration for Iceberg tables\n",
    "!mkdir -p trino/catalog\n",
    "\n",
    "%%writefile trino/catalog/iceberg.properties\n",
    "connector.name=iceberg\n",
    "hive.metastore.uri=thrift://hive-metastore:9083\n",
    "iceberg.catalog.type=hive_metastore\n",
    "iceberg.file-format=PARQUET\n",
    "hive.s3.endpoint=http://minio:9000\n",
    "hive.s3.path-style-access=true\n",
    "hive.s3.aws-access-key=minio\n",
    "hive.s3.aws-secret-key=minio123\n",
    "\n",
    "# Example: Iceberg table schema definition\n",
    "print(\"\"\"Example Iceberg table creation in Trino:\"\"\")\n",
    "print(\"\"\"CREATE SCHEMA IF NOT EXISTS iceberg.mta_data;\n",
    "\n",
    "CREATE TABLE iceberg.mta_data.vehicle_positions (\n",
    "    vehicle_id VARCHAR,\n",
    "    trip_id VARCHAR,\n",
    "    route_id VARCHAR,\n",
    "    timestamp TIMESTAMP,\n",
    "    latitude DOUBLE,\n",
    "    longitude DOUBLE,\n",
    "    speed DOUBLE,\n",
    "    bearing DOUBLE\n",
    ") WITH (\n",
    "    format = 'PARQUET',\n",
    "    partitioning = ARRAY['day(timestamp)']\n",
    ");\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcea4e7",
   "metadata": {},
   "source": [
    "## 6. Query Data with Trino\n",
    "\n",
    "Use Trino to query the Iceberg tables for real-time and historical analytics. Write SQL queries to retrieve insights such as bus delays and bunching events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3815573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Query with Trino using Python client\n",
    "print(\"NOTE: This requires Trino to be running properly. The code below shows how to query.\")\n",
    "print(\"Install the client with: pip install trino\\n\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "trino_example_code = \"\"\"\n",
    "import trino\n",
    "import pandas as pd\n",
    "\n",
    "def query_trino():\n",
    "    # Connect to Trino\n",
    "    conn = trino.dbapi.connect(\n",
    "        host='localhost',\n",
    "        port=8080,\n",
    "        user='trino',\n",
    "        catalog='iceberg',\n",
    "        schema='mta_data',\n",
    "    )\n",
    "    \n",
    "    # Example query\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            route_id,\n",
    "            date_trunc('hour', timestamp) as hour,\n",
    "            count(*) as position_count,\n",
    "            avg(speed) as avg_speed,\n",
    "            min(speed) as min_speed,\n",
    "            max(speed) as max_speed\n",
    "        FROM vehicle_positions\n",
    "        WHERE timestamp >= current_timestamp - interval '1' day\n",
    "        GROUP BY 1, 2\n",
    "        ORDER BY 1, 2\n",
    "        LIMIT 10\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute query and convert to DataFrame\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(query)\n",
    "    columns = [desc[0] for desc in cur.description]\n",
    "    df = pd.DataFrame(cur.fetchall(), columns=columns)\n",
    "    return df\n",
    "\n",
    "# Run query and display results\n",
    "try:\n",
    "    results_df = query_trino()\n",
    "    print(results_df)\n",
    "except Exception as e:\n",
    "    print(f\"Error querying Trino: {e}\")\n",
    "\"\"\"\n",
    "\n",
    "print(trino_example_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedd84ca",
   "metadata": {},
   "source": [
    "## 7. Visualize Real-Time Data in Superset\n",
    "\n",
    "Create a Superset dashboard to visualize real-time bus locations, headway variance, and alerts. Include auto-refreshing charts and maps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7258a51",
   "metadata": {},
   "source": [
    "**Instructions for Setting Up Superset:**\n",
    "\n",
    "1. Connect Superset to your Trino/Iceberg backend.\n",
    "2. Add the `vehicle_positions` and other relevant tables as datasets.\n",
    "3. Create charts (e.g., map, time series) to visualize bus locations and metrics.\n",
    "4. Build a dashboard and enable auto-refresh for real-time updates.\n",
    "\n",
    "For local development, you can add Superset to your Docker Compose file with:\n",
    "\n",
    "```yaml\n",
    "  superset:\n",
    "    image: apache/superset:latest\n",
    "    container_name: superset\n",
    "    ports:\n",
    "      - \"8088:8088\"\n",
    "    environment:\n",
    "      - SUPERSET_SECRET_KEY=your_secret_key\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb50cea1",
   "metadata": {},
   "source": [
    "# Building a Modern Data Engineering Portfolio: Local-First vs Cloud-First Pipeline\n",
    "\n",
    "This section demonstrates how to build a comprehensive data engineering project using both local-first (DuckDB/MotherDuck) and cloud-first (AWS, GCP, Azure) approaches. We'll use the NYC Taxi Trips dataset as an example, and walk through ingestion, transformation, storage, querying, and portfolio presentation best practices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecfb23e",
   "metadata": {},
   "source": [
    "## Step 1: Project Setup and Data Ingestion\n",
    "\n",
    "- Install Python 3, DuckDB, and optionally Docker for local development.\n",
    "- Set up cloud resources (S3, GCS, ADLS) and CLIs for AWS, GCP, Azure.\n",
    "- Download sample NYC Taxi data (yellow/green trip records) as CSV files.\n",
    "- Use ELT: Extract (download), Load (to DuckDB/cloud), Transform (later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460ad60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for data\n",
    "!mkdir -p data/raw data/parquet\n",
    "\n",
    "# Download sample data (NYC Yellow Taxi, Jan 2021)\n",
    "!wget -nc https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz -P data/raw/ -q\n",
    "print(\"Downloaded NYC Taxi data to data/raw/yellow_tripdata_2021-01.csv.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3187938",
   "metadata": {},
   "source": [
    "## Step 2: Local Data Storage with DuckDB/MotherDuck\n",
    "\n",
    "- Create a DuckDB database and load CSV data.\n",
    "- Optionally, convert to Parquet for interoperability.\n",
    "- (Optional) Sync to MotherDuck for cloud access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08e6e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import os\n",
    "\n",
    "db_path = 'nyc_taxi.duckdb'\n",
    "data_file = 'data/raw/yellow_tripdata_2021-01.csv.gz'\n",
    "\n",
    "# Check if the data file exists\n",
    "if os.path.exists(data_file):\n",
    "    # Connect to DuckDB\n",
    "    con = duckdb.connect(db_path)\n",
    "    \n",
    "    # Create yellow_trips_raw table if it doesn't exist\n",
    "    if len(con.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='yellow_trips_raw'\").fetchall()) == 0:\n",
    "        print(f\"Creating yellow_trips_raw table from {data_file}...\")\n",
    "        con.execute(f\"CREATE TABLE yellow_trips_raw AS SELECT * FROM read_csv_auto('{data_file}');\")\n",
    "        print(f\"Table created with {con.execute('SELECT COUNT(*) FROM yellow_trips_raw').fetchone()[0]} rows\")\n",
    "    else:\n",
    "        print(\"Table yellow_trips_raw already exists\")\n",
    "        print(f\"It contains {con.execute('SELECT COUNT(*) FROM yellow_trips_raw').fetchone()[0]} rows\")\n",
    "    \n",
    "    # Export sample to Parquet (if it doesn't exist)\n",
    "    parquet_file = 'data/parquet/yellow_tripdata_2021-01.parquet'\n",
    "    if not os.path.exists(parquet_file):\n",
    "        print(f\"Exporting to Parquet: {parquet_file}\")\n",
    "        con.execute(f\"COPY (SELECT * FROM yellow_trips_raw) TO '{parquet_file}' (FORMAT PARQUET, COMPRESSION ZSTD);\")\n",
    "        print(\"Export complete\")\n",
    "    else:\n",
    "        print(f\"Parquet file already exists: {parquet_file}\")\n",
    "    \n",
    "    # Show schema and sample data\n",
    "    print(\"\\nTable Schema:\")\n",
    "    print(con.execute(\"DESCRIBE yellow_trips_raw\").fetchdf())\n",
    "    \n",
    "    print(\"\\nSample Data:\")\n",
    "    print(con.execute(\"SELECT * FROM yellow_trips_raw LIMIT 5\").fetchdf())\n",
    "    \n",
    "    con.close()\n",
    "else:\n",
    "    print(f\"Data file not found: {data_file}\")\n",
    "    print(\"Please run the previous cell to download the data first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75a71a7",
   "metadata": {},
   "source": [
    "## Step 3: Cloud Data Storage & Querying Setup\n",
    "\n",
    "- AWS: Upload Parquet to S3, define Athena table.\n",
    "- GCP: Upload to GCS, load into BigQuery or create external table.\n",
    "- Azure: Upload to ADLS, create Synapse external table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b4e4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables for AWS credentials\n",
    "load_dotenv()\n",
    "\n",
    "def upload_to_s3(local_file, bucket_name, s3_key):\n",
    "    print(f\"This function would upload {local_file} to s3://{bucket_name}/{s3_key}\")\n",
    "    print(\"To actually run this, you need to:\")\n",
    "    print(\"1. Set AWS credentials in a .env file or environment variables\")\n",
    "    print(\"2. Create the S3 bucket\")\n",
    "    print(\"3. Uncomment the code below\")\n",
    "    \n",
    "    \"\"\"\n",
    "    # Create an S3 client\n",
    "    s3_client = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "        aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "        region_name=os.getenv('AWS_REGION', 'us-east-1')\n",
    "    )\n",
    "    \n",
    "    # Upload the file\n",
    "    try:\n",
    "        s3_client.upload_file(local_file, bucket_name, s3_key)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading to S3: {e}\")\n",
    "        return False\n",
    "    \"\"\"\n",
    "    return True\n",
    "\n",
    "# Example usage\n",
    "parquet_file = 'data/parquet/yellow_tripdata_2021-01.parquet'\n",
    "bucket_name = 'my-taxi-data-bucket'\n",
    "s3_key = 'tripdata/yellow_tripdata_2021-01.parquet'\n",
    "\n",
    "if os.path.exists(parquet_file):\n",
    "    if upload_to_s3(parquet_file, bucket_name, s3_key):\n",
    "        print(f\"\\nTo query this data in AWS Athena, you would create an external table with:\")\n",
    "        print(\"\"\"\n",
    "        CREATE EXTERNAL TABLE yellow_trips (\n",
    "          VendorID INT,\n",
    "          tpep_pickup_datetime TIMESTAMP,\n",
    "          tpep_dropoff_datetime TIMESTAMP,\n",
    "          passenger_count INT,\n",
    "          trip_distance DOUBLE,\n",
    "          ...\n",
    "        )\n",
    "        STORED AS PARQUET\n",
    "        LOCATION 's3://my-taxi-data-bucket/tripdata/'\n",
    "        TBLPROPERTIES ('parquet.compression'='SNAPPY');\n",
    "        \"\"\")\n",
    "else:\n",
    "    print(f\"Parquet file not found: {parquet_file}\")\n",
    "    print(\"Please run the previous steps to create the Parquet file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea40334",
   "metadata": {},
   "source": [
    "## Step 4: Data Transformation and Analytics\n",
    "\n",
    "- Use DuckDB or dbt for local SQL transformations.\n",
    "- Use Athena/BigQuery/Synapse SQL for cloud.\n",
    "- Example: Clean, filter, and aggregate taxi trip data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8c9ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to the database\n",
    "con = duckdb.connect('nyc_taxi.duckdb')\n",
    "\n",
    "# Check if the raw table exists\n",
    "if len(con.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='yellow_trips_raw'\").fetchall()) > 0:\n",
    "    # Create a cleaned table with transformations\n",
    "    if len(con.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='yellow_trips_cleaned'\").fetchall()) == 0:\n",
    "        print(\"Creating cleaned table with transformations...\")\n",
    "        \n",
    "        # Note: Table columns might vary, adjust SQL as needed\n",
    "        try:\n",
    "            con.execute(\"\"\"\n",
    "            CREATE TABLE yellow_trips_cleaned AS\n",
    "            SELECT\n",
    "              VendorID,\n",
    "              tpep_pickup_datetime::TIMESTAMP as pickup_datetime,\n",
    "              tpep_dropoff_datetime::TIMESTAMP as dropoff_datetime,\n",
    "              passenger_count,\n",
    "              trip_distance,\n",
    "              RatecodeID,\n",
    "              PULocationID as pickup_location_id,\n",
    "              DOLocationID as dropoff_location_id,\n",
    "              payment_type,\n",
    "              fare_amount,\n",
    "              tip_amount,\n",
    "              total_amount,\n",
    "              EXTRACT(HOUR FROM tpep_pickup_datetime::TIMESTAMP) as pickup_hour,\n",
    "              DATE_TRUNC('day', tpep_pickup_datetime::TIMESTAMP) as pickup_date\n",
    "            FROM yellow_trips_raw\n",
    "            WHERE trip_distance > 0 \n",
    "              AND fare_amount > 0\n",
    "              AND passenger_count > 0\n",
    "              AND tpep_dropoff_datetime > tpep_pickup_datetime;\n",
    "            \"\"\")\n",
    "            \n",
    "            # Create an aggregated daily stats table\n",
    "            con.execute(\"\"\"\n",
    "            CREATE TABLE daily_trip_stats AS\n",
    "            SELECT\n",
    "              pickup_date,\n",
    "              COUNT(*) as trip_count,\n",
    "              AVG(trip_distance) as avg_distance,\n",
    "              AVG(fare_amount) as avg_fare,\n",
    "              AVG(tip_amount) as avg_tip,\n",
    "              SUM(total_amount) as total_revenue\n",
    "            FROM yellow_trips_cleaned\n",
    "            GROUP BY pickup_date\n",
    "            ORDER BY pickup_date;\n",
    "            \"\"\")\n",
    "            \n",
    "            print(\"Tables created successfully!\")\n",
    "            \n",
    "            # Show row counts\n",
    "            cleaned_count = con.execute(\"SELECT COUNT(*) FROM yellow_trips_cleaned\").fetchone()[0]\n",
    "            daily_count = con.execute(\"SELECT COUNT(*) FROM daily_trip_stats\").fetchone()[0]\n",
    "            \n",
    "            print(f\"yellow_trips_cleaned: {cleaned_count} rows\")\n",
    "            print(f\"daily_trip_stats: {daily_count} rows\")\n",
    "            \n",
    "            # Show daily stats sample\n",
    "            print(\"\\nDaily Trip Stats Sample:\")\n",
    "            print(con.execute(\"SELECT * FROM daily_trip_stats LIMIT 5\").fetchdf())\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating transformed tables: {e}\")\n",
    "            print(\"Table structure might be different, please check the raw data schema\")\n",
    "    else:\n",
    "        print(\"Transformed tables already exist\")\n",
    "        # Show sample data from transformed tables\n",
    "        print(\"\\nDaily Trip Stats Sample:\")\n",
    "        print(con.execute(\"SELECT * FROM daily_trip_stats LIMIT 5\").fetchdf())\n",
    "else:\n",
    "    print(\"Raw data table not found. Please load the data first.\")\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a635ff",
   "metadata": {},
   "source": [
    "## Step 5: Orchestration and Scheduling\n",
    "\n",
    "- Use Airflow, Prefect, or Kestra to automate ingestion, loading, and transformation.\n",
    "- Example: Airflow DAG for monthly data ingestion and transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf19a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile airflow_example.py\n",
    "# Example Airflow DAG for taxi data pipeline\n",
    "# This would be placed in your Airflow DAGs folder\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import duckdb\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'data_engineer',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "def load_to_duckdb(file_path, table_name):\n",
    "    \"\"\"Load CSV data to DuckDB\"\"\"\n",
    "    db_path = 'nyc_taxi.duckdb'\n",
    "    con = duckdb.connect(db_path)\n",
    "    con.execute(f\"CREATE TABLE IF NOT EXISTS {table_name} AS SELECT * FROM read_csv_auto('{file_path}');\")\n",
    "    con.close()\n",
    "    return f\"Loaded {file_path} to {table_name}\"\n",
    "\n",
    "def transform_data():\n",
    "    \"\"\"Transform raw taxi data\"\"\"\n",
    "    db_path = 'nyc_taxi.duckdb'\n",
    "    con = duckdb.connect(db_path)\n",
    "    \n",
    "    # Create cleaned table\n",
    "    con.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE yellow_trips_cleaned AS\n",
    "    SELECT\n",
    "      VendorID,\n",
    "      tpep_pickup_datetime::TIMESTAMP as pickup_datetime,\n",
    "      tpep_dropoff_datetime::TIMESTAMP as dropoff_datetime,\n",
    "      passenger_count,\n",
    "      trip_distance,\n",
    "      RatecodeID,\n",
    "      PULocationID as pickup_location_id,\n",
    "      DOLocationID as dropoff_location_id,\n",
    "      payment_type,\n",
    "      fare_amount,\n",
    "      tip_amount,\n",
    "      total_amount\n",
    "    FROM yellow_trips_raw\n",
    "    WHERE trip_distance > 0 AND fare_amount > 0;\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create aggregated stats\n",
    "    con.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE daily_trip_stats AS\n",
    "    SELECT\n",
    "      DATE_TRUNC('day', pickup_datetime) as pickup_date,\n",
    "      COUNT(*) as trip_count,\n",
    "      AVG(trip_distance) as avg_distance,\n",
    "      AVG(fare_amount) as avg_fare,\n",
    "      SUM(total_amount) as total_revenue\n",
    "    FROM yellow_trips_cleaned\n",
    "    GROUP BY pickup_date\n",
    "    ORDER BY pickup_date;\n",
    "    \"\"\")\n",
    "    \n",
    "    con.close()\n",
    "    return \"Data transformation complete\"\n",
    "\n",
    "def export_to_parquet():\n",
    "    \"\"\"Export tables to Parquet format\"\"\"\n",
    "    db_path = 'nyc_taxi.duckdb'\n",
    "    con = duckdb.connect(db_path)\n",
    "    \n",
    "    # Export cleaned data\n",
    "    con.execute(\"COPY (SELECT * FROM yellow_trips_cleaned) TO 'data/parquet/yellow_trips_cleaned.parquet' (FORMAT PARQUET);\")\n",
    "    \n",
    "    # Export aggregated data\n",
    "    con.execute(\"COPY (SELECT * FROM daily_trip_stats) TO 'data/parquet/daily_trip_stats.parquet' (FORMAT PARQUET);\")\n",
    "    \n",
    "    con.close()\n",
    "    return \"Exported tables to Parquet format\"\n",
    "\n",
    "with DAG(\n",
    "    'taxi_data_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='Monthly taxi data processing pipeline',\n",
    "    schedule_interval='@monthly',\n",
    "    start_date=datetime(2021, 1, 1),\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "    \n",
    "    # Create directory structure\n",
    "    create_dirs = BashOperator(\n",
    "        task_id='create_directories',\n",
    "        bash_command='mkdir -p data/raw data/parquet'\n",
    "    )\n",
    "    \n",
    "    # Download taxi data\n",
    "    download_data = BashOperator(\n",
    "        task_id='download_data',\n",
    "        bash_command='''\n",
    "            YEAR={{ execution_date.strftime('%Y') }}\n",
    "            MONTH={{ execution_date.strftime('%m') }}\n",
    "            URL=\"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_${YEAR}-${MONTH}.csv.gz\"\n",
    "            wget -nc \"$URL\" -P data/raw/\n",
    "        '''\n",
    "    )\n",
    "    \n",
    "    # Load data to DuckDB\n",
    "    load_data = PythonOperator(\n",
    "        task_id='load_to_duckdb',\n",
    "        python_callable=load_to_duckdb,\n",
    "        op_kwargs={\n",
    "            'file_path': 'data/raw/yellow_tripdata_{{ execution_date.strftime(\"%Y-%m\") }}.csv.gz',\n",
    "            'table_name': 'yellow_trips_raw'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Transform data\n",
    "    transform = PythonOperator(\n",
    "        task_id='transform_data',\n",
    "        python_callable=transform_data\n",
    "    )\n",
    "    \n",
    "    # Export to Parquet\n",
    "    export = PythonOperator(\n",
    "        task_id='export_to_parquet',\n",
    "        python_callable=export_to_parquet\n",
    "    )\n",
    "    \n",
    "    # Define task dependencies\n",
    "    create_dirs >> download_data >> load_data >> transform >> export\n",
    "\n",
    "print(\"Example Airflow DAG has been written to airflow_example.py\")\n",
    "print(\"To use this, you would need to install Apache Airflow and place this file in your DAGs folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ce6f8e",
   "metadata": {},
   "source": [
    "## Step 6: Comparing Local-First and Cloud-First Environments\n",
    "\n",
    "| Aspect | Local-First (DuckDB/MotherDuck) | Cloud-First (AWS/GCP/Azure) |\n",
    "|--------|-------------------------------|-----------------------------|\n",
    "| Data Size | Small to medium (<100GB) | Medium to huge (TBs+) |\n",
    "| Performance | Fast for analytics, no network | Scales for big data, some latency |\n",
    "| Cost | Lowest, free/cheap | Pay per query/storage |\n",
    "| Complexity | Very low, no servers | Moderate, cloud setup |\n",
    "| Use Cases | Local dev, prototyping, EDA | Production, multi-user, big data |\n",
    "| Orchestration | Simple scripts, cron, Airflow | Cloud-native (Step Functions, Cloud Composer) |\n",
    "| Maintenance | Manual updates | Managed services |\n",
    "| Security | Limited, local files | IAM, encryption, VPC |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb5a302",
   "metadata": {},
   "source": [
    "## Step 7: Portfolio Presentation Tips\n",
    "\n",
    "- Organize your repo: data/, notebooks/, scripts/, dbt/, airflow_dags/, infrastructure/.\n",
    "- Write a clear README with overview, architecture diagram, setup guide, results, and findings.\n",
    "- Include visualizations (charts, maps) and discuss insights.\n",
    "- Summarize pros/cons of each approach and suggest future work.\n",
    "- Invite readers to reproduce your results and explore the project.\n",
    "\n",
    "### Repository Organization Example\n",
    "\n",
    "```\n",
    "data_engineering_realtime_pipeline/\n",
    "├── README.md                    # Project overview, setup guide, findings\n",
    "├── pyproject.toml               # Dependencies and project configuration\n",
    "├── docker-compose.yml           # Local infrastructure setup\n",
    "├── mta_gtfs_realtime_prediction.ipynb  # GTFS-RT pipeline notebook\n",
    "├── nyc_taxi_pipeline.ipynb      # NYC Taxi pipeline notebook\n",
    "├── .env.example                 # Template for environment variables\n",
    "├── data/                        # Data directory (git-ignored)\n",
    "│   ├── raw/                     # Raw data files\n",
    "│   └── parquet/                 # Processed Parquet files\n",
    "├── dbt/                         # dbt models for transformations\n",
    "├── airflow_dags/                # Airflow DAG definitions\n",
    "├── scripts/                     # Utility scripts\n",
    "│   ├── download_data.py         # Data download script\n",
    "│   └── process_taxi_data.py     # Data processing script\n",
    "├── flink_jobs/                  # Apache Flink job definitions\n",
    "│   └── vehicle_metrics.py       # Flink job for processing vehicle data\n",
    "└── trino/                       # Trino configuration\n",
    "    └── catalog/                 # Catalog definitions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5d05e3",
   "metadata": {},
   "source": [
    "Below is a minimal, self-contained **producer** you can drop into your repo to poll the GTFS-Realtime feed on a configurable interval and push each update to Kafka.  \n",
    "\n",
    "---\n",
    "## 1. Update your `.env.example`\n",
    "Add these new variables (so your local `.env` can override them):\n",
    "```ini\n",
    "# .env.example (add at the bottom)\n",
    "KAFKA_BROKER=localhost:29092\n",
    "KAFKA_TOPIC=vehicle_positions\n",
    "POLL_INTERVAL=15      # seconds between pulls\n",
    "MTA_FEED_URL=https://gtfsrt.prod.obanyc.com/vehiclePositions?key=${MTA_API_KEY}\n",
    "```\n",
    "---\n",
    "## 2. Install the new dependency\n",
    "Make sure your `pyproject.toml` or `requirements.txt` includes:\n",
    "```\n",
    "confluent-kafka\n",
    "```\n",
    "Then run:\n",
    "```bash\n",
    "pip install confluent-kafka\n",
    "```\n",
    "---\n",
    "## 3. Add a streaming script\n",
    "Create a file at `scripts/stream_gtfs.py`:\n",
    "```python\n",
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import requests\n",
    "from google.protobuf import json_format\n",
    "from google.transit import gtfs_realtime_pb2\n",
    "from confluent_kafka import Producer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s: %(message)s\")\n",
    "\n",
    "def main():\n",
    "    load_dotenv()  # finds .env in project root\n",
    "    api_url      = os.getenv(\"MTA_FEED_URL\")\n",
    "    broker       = os.getenv(\"KAFKA_BROKER\")\n",
    "    topic        = os.getenv(\"KAFKA_TOPIC\", \"vehicle_positions\")\n",
    "    interval_sec = int(os.getenv(\"POLL_INTERVAL\", \"15\"))\n",
    "\n",
    "    if not api_url or not broker:\n",
    "        logging.error(\"Missing MTA_FEED_URL or KAFKA_BROKER in environment.\")\n",
    "        return\n",
    "\n",
    "    # Configure Kafka producer\n",
    "    producer = Producer({\"bootstrap.servers\": broker})\n",
    "    logging.info(f\"Starting GTFS-RT streamer → Kafka {broker} topic '{topic}' every {interval_sec}s\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            resp = requests.get(api_url, timeout=10)\n",
    "            resp.raise_for_status()\n",
    "            feed = gtfs_realtime_pb2.FeedMessage()\n",
    "            feed.ParseFromString(resp.content)\n",
    "\n",
    "            batch = []\n",
    "            for entity in feed.entity:\n",
    "                # Convert each protobuf entity to a JSONable dict\n",
    "                obj = json_format.MessageToDict(entity)\n",
    "                batch.append(obj)\n",
    "\n",
    "            if batch:\n",
    "                for record in batch:\n",
    "                    producer.produce(topic, key=record.get(\"id\", \"\"), value=producer._serialize(record))\n",
    "                producer.flush()\n",
    "                logging.info(f\"Published {len(batch)} records to '{topic}'\")\n",
    "            else:\n",
    "                logging.info(\"No entities in feed this cycle\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching/publishing GTFS-RT data: {e}\")\n",
    "\n",
    "        time.sleep(interval_sec)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "**A few notes:**\n",
    "1. It reads your **full feed URL** (including `?key=…`) from `MTA_FEED_URL`.  \n",
    "2. It polls every `POLL_INTERVAL` seconds.  \n",
    "3. It pushes each entity as a JSON message to the `vehicle_positions` topic.  \n",
    "---\n",
    "## 4. (Optional) Wire it into Docker Compose\n",
    "If you’d rather run this as a containerized service, add to your `docker-compose.yml`:\n",
    "```yaml\n",
    "  gtfs-producer:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: Dockerfile.producer\n",
    "    env_file: .env\n",
    "    depends_on:\n",
    "      - kafka\n",
    "    networks:\n",
    "      - default\n",
    "```\n",
    "And create a `Dockerfile.producer`:\n",
    "```dockerfile\n",
    "FROM python:3.11-slim\n",
    "WORKDIR /app\n",
    "COPY pyproject.toml poetry.lock* /app/\n",
    "RUN pip install --no-cache-dir confluent-kafka python-dotenv google-protobuf requests\n",
    "COPY scripts/stream_gtfs.py /app/\n",
    "CMD [\"python\", \"stream_gtfs.py\"]\n",
    "```\n",
    "Then `docker-compose up -d gtfs-producer` will start your real-time stream alongside Kafka.\n",
    "---\n",
    "## 5. Run it locally\n",
    "1. Fill out your `.env` from `.env.example`.  \n",
    "2. Start Kafka (`docker-compose up -d zookeeper kafka`).  \n",
    "3. In one terminal, run:\n",
    "   ```bash\n",
    "   python scripts/stream_gtfs.py\n",
    "   ```\n",
    "4. In another, consume from Kafka to verify:\n",
    "   ```bash\n",
    "   kafka-console-consumer --bootstrap-server localhost:29092 --topic vehicle_positions --from-beginning\n",
    "   ```\n",
    "\n",
    "That’s it—now you have continuous real-time ingestion of MTA BusTime GTFS-Realtime data into Kafka, ready for downstream Flink, Iceberg, Trino, Superset, or whatever comes next in your pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2b84d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/stream_gtfs.py\n",
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import requests\n",
    "from google.protobuf import json_format\n",
    "from google.transit import gtfs_realtime_pb2\n",
    "from confluent_kafka import Producer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s: %(message)s\")\n",
    "\n",
    "def main():\n",
    "    load_dotenv()  # finds .env in project root\n",
    "    api_url      = os.getenv(\"MTA_FEED_URL\")\n",
    "    broker       = os.getenv(\"KAFKA_BROKER\")\n",
    "    topic        = os.getenv(\"KAFKA_TOPIC\", \"vehicle_positions\")\n",
    "    interval_sec = int(os.getenv(\"POLL_INTERVAL\", \"15\"))\n",
    "\n",
    "    if not api_url or not broker:\n",
    "        logging.error(\"Missing MTA_FEED_URL or KAFKA_BROKER in environment.\")\n",
    "        return\n",
    "\n",
    "    # Configure Kafka producer\n",
    "    producer = Producer({\"bootstrap.servers\": broker})\n",
    "    logging.info(f\"Starting GTFS-RT streamer → Kafka {broker} topic '{topic}' every {interval_sec}s\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            resp = requests.get(api_url, timeout=10)\n",
    "            resp.raise_for_status()\n",
    "            feed = gtfs_realtime_pb2.FeedMessage()\n",
    "            feed.ParseFromString(resp.content)\n",
    "\n",
    "            batch = []\n",
    "            for entity in feed.entity:\n",
    "                # Convert each protobuf entity to a JSONable dict\n",
    "                obj = json_format.MessageToDict(entity)\n",
    "                batch.append(obj)\n",
    "\n",
    "            if batch:\n",
    "                for record in batch:\n",
    "                    producer.produce(topic, key=record.get(\"id\", \"\"), value=producer._serialize(record))\n",
    "                producer.flush()\n",
    "                logging.info(f\"Published {len(batch)} records to '{topic}'\")\n",
    "            else:\n",
    "                logging.info(\"No entities in feed this cycle\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching/publishing GTFS-RT data: {e}\")\n",
    "\n",
    "        time.sleep(interval_sec)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
