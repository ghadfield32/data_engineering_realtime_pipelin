Metadata-Version: 2.4
Name: data_engineering_realtime_pipeline
Version: 0.1.0
Summary: Multi-cloud data engineering pipeline framework supporting multiple orchestrators and data sources
Author-email: Your Name <your.email@example.com>
License: MIT
Project-URL: Source Code, https://github.com/yourusername/data_engineering_realtime_pipeline
Project-URL: Bug Tracker, https://github.com/yourusername/data_engineering_realtime_pipeline/issues
Project-URL: Documentation, https://github.com/yourusername/data_engineering_realtime_pipeline/blob/main/README.md
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Topic :: Software Development :: Libraries
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: requests>=2.31.0
Requires-Dist: pydantic>=2.5.3
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: pandas>=2.0.0
Requires-Dist: pyarrow>=10.0.0
Requires-Dist: duckdb>=0.9.0
Requires-Dist: gtfs-realtime-bindings>=1.0.0
Requires-Dist: protobuf>=4.24.4
Requires-Dist: confluent-kafka>=2.1.0
Requires-Dist: sqlalchemy<2.0.0,>=1.4.49
Requires-Dist: psycopg2-binary>=2.9.0
Requires-Dist: fastavro>=1.7.0
Provides-Extra: aws
Requires-Dist: boto3>=1.28.0; extra == "aws"
Requires-Dist: awscli>=1.29.0; extra == "aws"
Provides-Extra: gcp
Requires-Dist: google-cloud-storage>=2.10.0; extra == "gcp"
Requires-Dist: google-cloud-bigquery>=3.12.0; extra == "gcp"
Requires-Dist: google-auth>=2.22.0; extra == "gcp"
Provides-Extra: azure
Requires-Dist: azure-storage-blob>=12.19.0; extra == "azure"
Requires-Dist: azure-identity>=1.15.0; extra == "azure"
Provides-Extra: airflow
Requires-Dist: apache-airflow>=3.0.0; extra == "airflow"
Requires-Dist: apache-airflow-providers-postgres>=5.0.0; extra == "airflow"
Requires-Dist: apache-airflow-providers-apache-kafka>=1.1.0; extra == "airflow"
Requires-Dist: apache-airflow-providers-dbt-cloud>=3.5.0; extra == "airflow"
Requires-Dist: apache-airflow-providers-slack>=8.0.0; extra == "airflow"
Requires-Dist: apache-airflow-providers-http>=4.5.0; extra == "airflow"
Requires-Dist: apache-airflow-providers-amazon>=6.0.0; extra == "airflow"
Requires-Dist: apache-airflow-providers-google>=10.0.0; extra == "airflow"
Requires-Dist: apache-airflow-providers-microsoft-azure>=6.0.0; extra == "airflow"
Requires-Dist: apache-airflow-providers-standard>=1.0.0; extra == "airflow"
Provides-Extra: dbt
Requires-Dist: dbt-core>=1.5.0; extra == "dbt"
Requires-Dist: dbt-duckdb>=1.5.0; extra == "dbt"
Requires-Dist: dbt-postgres>=1.5.0; extra == "dbt"
Provides-Extra: kestra
Requires-Dist: requests>=2.31.0; extra == "kestra"
Requires-Dist: pyyaml>=6.0; extra == "kestra"
Provides-Extra: avro
Requires-Dist: fastavro>=1.7.0; extra == "avro"
Provides-Extra: kafka
Requires-Dist: confluent-kafka>=2.1.0; extra == "kafka"
Provides-Extra: dev
Requires-Dist: black>=23.3.0; extra == "dev"
Requires-Dist: mypy>=1.3.0; extra == "dev"
Requires-Dist: pytest>=7.3.1; extra == "dev"
Requires-Dist: pytest-cov>=4.1.0; extra == "dev"
Requires-Dist: ruff>=0.0.271; extra == "dev"
Requires-Dist: flake8>=6.0.0; extra == "dev"
Requires-Dist: pre-commit>=3.3.3; extra == "dev"
Requires-Dist: invoke>=2.0.0; extra == "dev"
Requires-Dist: python-dotenv>=1.0.0; extra == "dev"
Provides-Extra: development
Requires-Dist: data_engineering_realtime_pipeline[airflow,avro,aws,azure,dbt,dev,gcp,kafka]; extra == "development"
Requires-Dist: jupyter>=1.0.0; extra == "development"
Requires-Dist: ipython>=8.12.0; extra == "development"
Provides-Extra: staging
Requires-Dist: data_engineering_realtime_pipeline[airflow,avro,aws,azure,dbt,gcp,kafka]; extra == "staging"
Provides-Extra: production
Requires-Dist: data_engineering_realtime_pipeline[airflow,avro,aws,azure,dbt,gcp,kafka]; extra == "production"
Provides-Extra: airflow-dev
Requires-Dist: apache-airflow>=3.0.0; extra == "airflow-dev"
Requires-Dist: data_engineering_realtime_pipeline[avro,aws,azure,dbt,dev,gcp,kafka]; extra == "airflow-dev"
Provides-Extra: kestra-dev
Requires-Dist: data_engineering_realtime_pipeline[avro,aws,azure,dbt,dev,gcp,kafka,kestra]; extra == "kestra-dev"
Provides-Extra: all
Requires-Dist: data_engineering_realtime_pipeline[airflow,avro,aws,azure,dbt,dev,gcp,kafka,kestra]; extra == "all"

# Data Engineering Real-Time Pipeline

A modular, end-to-end data pipeline framework supporting multiple orchestrators (Airflow 3.0, Airflow 2.x, Kestra) and data sources. This framework integrates with a variety of tools including dbt, Kafka, MinIO, Trino, and DuckDB to provide a flexible and comprehensive data engineering solution.

## Features

- **Multiple Orchestrators**: 
  - Apache Airflow 3.0 with AssetWatcher (event-driven triggers)
  - Apache Airflow 2.x compatibility
  - Kestra workflow engine (declarative YAML-based pipeline definition)

- **Streaming Capabilities**:
  - Kafka integration for real-time data processing
  - Flink for stream processing

- **Data Storage**:
  - PostgreSQL for relational data
  - DuckDB for analytics
  - MinIO (S3-compatible) for object storage
  - Trino for federated queries

- **Transformation**:
  - dbt models for data transformations
  - SQL-based data modeling

- **Infrastructure as Code**:
  - Docker Compose for local development
  - Terraform for cloud deployment
  - GitHub Actions for CI/CD

## Quick Start

### Prerequisites

- Docker and Docker Compose
- Python 3.10+
- Git

### Setup Options

Choose the environment that matches your needs:

#### General Development Environment
```bash
# On Linux/macOS
./scripts/setup_dev_environment.sh

# On Windows PowerShell
.\scripts\setup_dev_environment.ps1
```

#### Airflow 3.0 Environment
```bash
# On Linux/macOS
./scripts/setup_airflow3_environment.sh

# On Windows PowerShell
# Use equivalent PowerShell script
```

#### Kestra Environment
```bash
# On Linux/macOS
./scripts/setup_kestra_environment.sh

# On Windows PowerShell
# Use equivalent PowerShell script
```

### Run with Docker Compose

Start all services:

```bash
docker-compose up -d
```

Access the different services:
- Airflow UI: http://localhost:8081
- Kestra UI: http://localhost:8083
- Trino UI: http://localhost:8080
- MinIO Console: http://localhost:9001
- Kafka UI: http://localhost:8089

## Architecture

The pipeline supports multiple data sources and destinations through a common framework:

```
Data Sources                Orchestration               Data Destination
┌───────────┐               ┌───────────┐               ┌───────────┐
│  GTFS RT  │──┐            │           │            ┌──│PostgreSQL │
└───────────┘  │            │           │            │  └───────────┘
               │            │           │            │
┌───────────┐  │            │  Airflow  │            │  ┌───────────┐
│  NBA API  │──┼────────────┤    or     ├────────────┼──│  DuckDB   │
└───────────┘  │            │  Kestra   │            │  └───────────┘
               │            │           │            │
┌───────────┐  │            │           │            │  ┌───────────┐
│  Weather  │──┘            │           │            └──│   MinIO   │
└───────────┘               └───────────┘               └───────────┘
```

## Project Structure

```
data_engineering_realtime_pipelin/
├── src/
│   ├── orchestrators/
│   │   ├── airflow/           # Airflow DAGs and config
│   │   ├── dbt/               # dbt models 
│   │   └── kestra/            # Kestra flows
│   ├── infrastructure/
│   │   └── docker/            # Docker files and configs
├── terraform/                 # IaC for cloud deployment
├── scripts/                   # Setup and utility scripts
├── docker-compose.yml         # Local development services
└── pyproject.toml             # Project dependencies
```

## Environments and Dependencies

This project uses `pyproject.toml` to manage dependencies for different environments:

- **Development**: `pip install -e ".[development]"`
- **Airflow 3.0**: `pip install -e ".[airflow3_dev]"`
- **Airflow 2.x**: `pip install -e ".[airflow2_dev]"`
- **Kestra**: `pip install -e ".[kestra_dev]"`

Each environment has its own setup script in the `scripts/` directory.

## Running Pipelines

### Airflow Pipelines

The project includes several example DAGs:

- `nba_ingest_pipeline.py`: Processes NBA data with AssetWatcher
- `weather_kafka_pipeline.py`: Processes weather data from Kafka
- `gtfs_data_pipeline.py`: Processes transit data

To trigger a pipeline, either:
1. Wait for the scheduled execution
2. Trigger manually from the Airflow UI
3. Create the appropriate event file (for AssetWatcher DAGs)

#### Triggering DAGs from the Airflow UI

Once your Airflow webserver and scheduler are running (e.g., via Docker Compose), triggering a DAG is entirely handled through the **Airflow UI**:

##### 1. Accessing the Airflow UI

1. Ensure your `airflow-webserver` and `airflow-scheduler` containers are **healthy** and **running**:

   ```bash
   docker-compose up -d airflow-webserver airflow-scheduler
   ```

   Confirm via:

   ```bash
   docker ps
   ```

2. Open your browser to **[http://localhost:8080/](http://localhost:8080/)** (or the port specified in your configuration).
   The default login is `admin`/`admin` for most Docker setups.

##### 2. Navigating to the DAGs List

* Click **DAGs** in the left sidebar to open the **DAG List View**.
* You'll see a list of available DAGs with their status, schedule, and actions.
* Use the **Pause/Play toggle** to pause or unpause a DAG. Paused DAGs won't execute on schedule but can still be triggered manually.

##### 3. Triggering a DAG Manually

###### Simple Trigger:
1. In the **Links** column of your DAG row, click the **▶** (Trigger DAG) icon.
2. A confirmation modal appears; click **Trigger**.
3. This enqueues an immediate **DAG run** outside of its schedule.

###### Trigger with Configuration:
If your DAG accepts parameters:

1. Ensure `core.dag_run_conf_overrides_params = True` in `airflow.cfg` to allow UI overrides.
2. Click the **•••** menu and select **Trigger DAG w/ config**.
3. Fill out the JSON configuration form that appears—e.g.,:

   ```json
   {
     "run_date": "2025-05-01T00:00:00",
     "batch_size": 500
   }
   ```
4. Click **Trigger** to start a parameterized run.

##### 4. Monitoring the Triggered Run

After triggering, you can track your run via:

1. **Runs Tab**
   * Click on your DAG name to open the **DAG Details Page**.
   * Select the **Runs** tab to see your newly created run, its **Run Type** (Manual), **Logical Date**, and **State**.

2. **Grid View**
   * In **Grid** or **Graph** view, click the column corresponding to your run.
   * Colored cells indicate task status (Success, Running, Failed). Hover to see logs or clear retries.

##### 5. Tips and CLI Alternatives

* If you need to use the CLI inside Docker:

  ```bash
  docker exec -it airflow-webserver airflow dags list
  docker exec -it airflow-webserver airflow dags trigger <DAG_ID>
  ```

* **Asset-Triggered DAGs**: Asset-driven DAGs show a **dataset icon** instead of ▶; clicking the icon surfaces asset runs.
* **Auditability**: Manual runs are tagged as `manual__<timestamp>` in the **Run ID** column.

### Kestra Pipelines

Kestra flows are defined in YAML and are located in `src/orchestrators/kestra/flows/`.

To run a Kestra flow:
1. Access the Kestra UI at http://localhost:8083
2. Navigate to the flow you want to run
3. Click "Execute"

### dbt Transformations

dbt models are run either:
1. As part of a pipeline (Airflow or Kestra)
2. Directly using the dbt command line:

```bash
cd src/orchestrators/dbt
dbt run --profiles-dir .
```

## Contributing

1. Set up the development environment
2. Use pre-commit hooks to maintain code quality
3. Follow the single-responsibility principle for files
4. Add tests for new functionality

## License

MIT 
