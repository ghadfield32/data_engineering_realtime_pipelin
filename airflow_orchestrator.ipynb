{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src\\orchestrators\\learning-airflow\\README.md\n"
     ]
    }
   ],
   "source": [
    "%%writefile src\\orchestrators\\learning-airflow\\README.md\n",
    "\n",
    "\n",
    "# Airflow Learning Project\n",
    "\n",
    "This project demonstrates best practices for creating modular, maintainable Airflow DAGs using Airflow 3.0's TaskFlow API and AssetWatcher for event-driven scheduling.\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "```\n",
    "learning-airflow/\n",
    "├── dags/                     # DAG definition files \n",
    "│   ├── example_etl_galaxies.py     # Example ETL DAG that processes galaxy data\n",
    "│   ├── gtfs_data_pipeline.py       # DAG that processes GTFS realtime transit data\n",
    "│   ├── nba_ingest_pipeline.py      # DAG that processes NBA game data\n",
    "│   ├── user_metrics_etl.py         # Simple ETL DAG example for user metrics\n",
    "│   └── weather_kafka_pipeline.py   # DAG that processes weather data from Kafka\n",
    "├── include/                  # Shared code and resources\n",
    "│   ├── custom_functions/          # Modularized DAG functions\n",
    "│   │   ├── galaxy_functions.py     # Functions for galaxy data processing\n",
    "│   │   ├── gtfs_functions.py       # Functions for GTFS data processing\n",
    "│   │   ├── nba_functions.py        # Functions for NBA data processing\n",
    "│   │   ├── user_metrics_functions.py # Functions for user metrics processing\n",
    "│   │   └── weather_functions.py    # Functions for weather data processing\n",
    "│   ├── data/                      # Sample data files\n",
    "│   └── astronomy.db               # DuckDB database for the example DAG\n",
    "├── tests/                    # Test files\n",
    "├── .astro/                   # Astro CLI configuration\n",
    "├── Dockerfile                # Custom Dockerfile for this project\n",
    "├── packages.txt              # System-level dependencies\n",
    "├── README.md                 # This file\n",
    "└── requirements.txt          # Python dependencies\n",
    "```\n",
    "\n",
    "## DAG Modularization\n",
    "\n",
    "Each DAG in this project follows best practices for code organization:\n",
    "\n",
    "1. **Separation of concerns**: Core data processing logic is separated from DAG flow definition\n",
    "2. **Modular functions**: Each DAG has corresponding helper functions in `include/custom_functions/`\n",
    "3. **Reusable components**: Common patterns are extracted into helper classes\n",
    "4. **Well-documented**: DAGs and functions include descriptive docstrings\n",
    "\n",
    "## Example DAGs\n",
    "\n",
    "### Galaxy ETL Example\n",
    "\n",
    "A simple ETL pipeline that extracts galaxy data, filters based on distance, and loads into a DuckDB database.\n",
    "\n",
    "```python\n",
    "# Use like this:\n",
    "from include.custom_functions.galaxy_functions import get_galaxy_data\n",
    "```\n",
    "\n",
    "### GTFS Data Pipeline\n",
    "\n",
    "Processes GTFS Realtime transit data with different storage backends (S3, BigQuery, Azure, DuckDB).\n",
    "\n",
    "```python\n",
    "# Use like this:\n",
    "from include.custom_functions.gtfs_functions import GTFSProcessor\n",
    "```\n",
    "\n",
    "### NBA Ingest Pipeline\n",
    "\n",
    "Demonstrates event-driven processing with AssetWatcher, fetching NBA game data and loading into PostgreSQL.\n",
    "\n",
    "```python\n",
    "# Use like this:\n",
    "from include.custom_functions.nba_functions import NBAProcessor\n",
    "```\n",
    "\n",
    "### User Metrics ETL\n",
    "\n",
    "Simple ETL example for processing user metrics.\n",
    "\n",
    "```python\n",
    "# Use like this:\n",
    "from include.custom_functions.user_metrics_functions import UserMetricsProcessor\n",
    "```\n",
    "\n",
    "### Weather Kafka Pipeline\n",
    "\n",
    "Shows how to consume Kafka messages, process weather data, and save to PostgreSQL with analytics.\n",
    "\n",
    "```python\n",
    "# Use like this:\n",
    "from include.custom_functions.weather_functions import WeatherProcessor\n",
    "```\n",
    "\n",
    "## Running the DAGs\n",
    "\n",
    "### Using Astro CLI\n",
    "\n",
    "This project is configured for the Astro CLI, making it easy to run locally:\n",
    "\n",
    "```bash\n",
    "# Start the project\n",
    "astro dev start\n",
    "\n",
    "# Access the Airflow UI\n",
    "# Open http://localhost:8080 in your browser\n",
    "# Default credentials: admin/admin\n",
    "```\n",
    "\n",
    "### Using Docker\n",
    "\n",
    "You can also run the project using Docker directly:\n",
    "\n",
    "```bash\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "### Environment Variables\n",
    "\n",
    "Environment variables can be set in a `.env` file at the project root or passed to the Airflow containers.\n",
    "\n",
    "## Testing\n",
    "\n",
    "Execute tests using the following commands:\n",
    "\n",
    "```bash\n",
    "# Run all tests\n",
    "astro dev pytest\n",
    "\n",
    "# Run a specific test\n",
    "astro dev pytest tests/dags/test_dag_example.py\n",
    "```\n",
    "\n",
    "## Contributing\n",
    "\n",
    "When adding a new DAG:\n",
    "\n",
    "1. Create a new DAG file in the `dags/` directory\n",
    "2. Create a module with helper functions in `include/custom_functions/`\n",
    "3. Add tests in the `tests/` directory\n",
    "4. Update this README with relevant information\n",
    "\n",
    "## License\n",
    "\n",
    "This project is licensed under the MIT License.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src\\orchestrators\\learning-airflow\\Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile src\\orchestrators\\learning-airflow\\Dockerfile\n",
    "FROM astrocrpublic.azurecr.io/runtime:3.0-2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!astro version\n",
    "\n",
    "# uncomment and cd to whereever you want to create airflow env\n",
    "#cd \n",
    "\n",
    "!astro dev init --from-template learning-airflow\n",
    "\n",
    "# or \n",
    "!mkdir hello-astro && cd hello-astro\n",
    "!astro dev init\n",
    "\n",
    "\n",
    "!astro dev start\n",
    "\n",
    "#Once healthy, open your browser to https://localhost:8080/ to access the Airflow UI. Your DAGs (including example-astronauts) appear under the DAGs view.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src\\orchestrators\\learning-airflow\\requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile src\\orchestrators\\learning-airflow\\requirements.txt\n",
    "\n",
    "# Astro Runtime includes the following pre-installed providers packages: https://docs.astronomer.io/astro/runtime-image-architecture#provider-packages\n",
    "\n",
    "apache-airflow[postgres]\n",
    "apache-airflow-providers-apache-kafka\n",
    "apache-airflow-providers-amazon\n",
    "apache-airflow-providers-google\n",
    "apache-airflow-providers-microsoft-azure\n",
    "apache-airflow-providers-postgres\n",
    "apache-airflow-providers-common-sql\n",
    "# apache-airflow-upgrade-check  # Tool for checking Airflow 3 upgrade compatibility\n",
    "# airflow upgrade_check\n",
    "\n",
    "duckdb>=1.0.0\n",
    "pandas>=2.0.0\n",
    "tabulate\n",
    "pyarrow>=10.0.0\n",
    "requests>=2.0.0\n",
    "python-dotenv>=1.0.0\n",
    "confluent-kafka>=2.0.0\n",
    "kafka-python\n",
    "protobuf\n",
    "gtfs-realtime-bindings\n",
    "psycopg2-binary\n",
    "ruff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src\\orchestrators\\learning-airflow\\dags\\example_etl_galaxies.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src\\orchestrators\\learning-airflow\\dags\\example_etl_galaxies.py\n",
    "\"\"\"\n",
    "## Galaxies ETL example DAG\n",
    "\n",
    "This example demonstrates an ETL pipeline using Airflow.\n",
    "The pipeline mocks data extraction for data about galaxies using a modularized\n",
    "function, filters the data based on the distance from the Milky Way, and loads the\n",
    "filtered data into a DuckDB database.\n",
    "\"\"\"\n",
    "  # This DAG uses the TaskFlow API. See: https://www.astronomer.io/docs/learn/airflow-decorators\n",
    "from airflow.sdk import Asset, chain, Param, dag, task\n",
    "from airflow.timetables.trigger import MultipleCronTriggerTimetable  # 🎉 Airflow 3 timetable\n",
    "from airflow.models import Variable  # 🔒 For centralized credentials\n",
    "from pendulum import datetime, duration\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# modularize code by importing functions from the include folder\n",
    "from include.custom_functions.galaxy_functions import get_galaxy_data\n",
    "\n",
    "# use the Airflow task logger to log information to the task logs (or use print())\n",
    "t_log = logging.getLogger(\"airflow.task\")\n",
    "\n",
    "# define variables used in a DAG as environment variables in .env for your whole Airflow instance\n",
    "# to standardize your DAGs\n",
    "_DUCKDB_INSTANCE_NAME = Variable.get(\"DUCKDB_INSTANCE_NAME\", default_var=os.getenv(\"DUCKDB_INSTANCE_NAME\", \"include/astronomy.db\"))  # 🔒 Use Variable.get for configuration\n",
    "_DUCKDB_TABLE_NAME = Variable.get(\"DUCKDB_TABLE_NAME\", default_var=os.getenv(\"DUCKDB_TABLE_NAME\", \"galaxy_data\"))  # 🔒 Use Variable.get for configuration\n",
    "_DUCKDB_TABLE_URI = f\"duckdb://{_DUCKDB_INSTANCE_NAME}/{_DUCKDB_TABLE_NAME}\"\n",
    "_CLOSENESS_THRESHOLD_LY_DEFAULT = Variable.get(\"CLOSENESS_THRESHOLD_LY_DEFAULT\", default_var=os.getenv(\"CLOSENESS_THRESHOLD_LY_DEFAULT\", 500000))  # 🔒 Use Variable.get for configuration\n",
    "_CLOSENESS_THRESHOLD_LY_PARAMETER_NAME = \"closeness_threshold_light_years\"\n",
    "_NUM_GALAXIES_TOTAL = Variable.get(\"NUM_GALAXIES_TOTAL\", default_var=os.getenv(\"NUM_GALAXIES_TOTAL\", 20))  # 🔒 Use Variable.get for configuration\n",
    "\n",
    "# -------------- #\n",
    "# DAG Definition #\n",
    "# -------------- #\n",
    "\n",
    "\n",
    "# instantiate a DAG with the @dag decorator and set DAG parameters (see: https://www.astronomer.io/docs/learn/airflow-dag-parameters)\n",
    "@dag(\n",
    "    start_date=datetime(2025, 4, 1),  # date after which the DAG can be scheduled\n",
    "    schedule=MultipleCronTriggerTimetable(  # 🎉 Using MultipleCronTriggerTimetable for multiple schedules\n",
    "        \"0 10 * * *\",  # 10 AM daily\n",
    "        \"0 14 * * 1-5\",  # 2 PM weekdays\n",
    "        timezone=\"UTC\"\n",
    "    ),\n",
    "    dag_display_name=\"Galaxy ETL 🚀\",  # 📌 dag_display_name improves UI discoverability\n",
    "    max_consecutive_failed_dag_runs=5,  # auto-pauses the DAG after 5 consecutive failed runs, experimental\n",
    "    max_active_runs=1,  # only allow one concurrent run of this DAG, prevents parallel DuckDB calls\n",
    "    doc_md=__doc__,  # add DAG Docs in the UI, see https://www.astronomer.io/docs/learn/custom-airflow-ui-docs-tutorial\n",
    "    default_args={\n",
    "        \"owner\": \"Astro\",  # owner of this DAG in the Airflow UI\n",
    "        \"retries\": 3,  # tasks retry 3 times before they fail\n",
    "        \"retry_delay\": duration(seconds=30),  # tasks wait 30s in between retries\n",
    "    },  # default_args are applied to all tasks in a DAG\n",
    "    tags=[\"example\", \"ETL\"],  # add tags in the UI\n",
    "    params={  # Airflow params can add interactive options on manual runs. See: https://www.astronomer.io/docs/learn/airflow-params\n",
    "        _CLOSENESS_THRESHOLD_LY_PARAMETER_NAME: Param(\n",
    "            _CLOSENESS_THRESHOLD_LY_DEFAULT,\n",
    "            type=\"number\",\n",
    "            title=\"Galaxy Closeness Threshold\",\n",
    "            description=\"Set how close galaxies need ot be to the milkyway in order to be loaded to DuckDB.\",\n",
    "        )\n",
    "    },\n",
    "    # Warning - in-memory DuckDB is not a persistent database between workers. To move this workflow in production, use a\n",
    "    # cloud-based database and based on concurrency capabilities adjust the two parameters below.\n",
    "    is_paused_upon_creation=False, # start running the DAG as soon as its created\n",
    ")\n",
    "def example_etl_galaxies():  # by default the dag_id is the name of the decorated function\n",
    "\n",
    "    # ---------------- #\n",
    "    # Task Definitions #\n",
    "    # ---------------- #\n",
    "    # the @task decorator turns any Python function into an Airflow task\n",
    "    # any @task decorated function that is called inside the @dag decorated\n",
    "    # function is automatically added to the DAG.\n",
    "    # if one exists for your use case you can still use traditional Airflow operators\n",
    "    # and mix them with @task decorators. Checkout registry.astronomer.io for available operators\n",
    "    # see: https://www.astronomer.io/docs/learn/airflow-decorators for information about @task\n",
    "    # see: https://www.astronomer.io/docs/learn/what-is-an-operator for information about traditional operators\n",
    "\n",
    "    @task(retries=2)  # you can override default_args at the task level\n",
    "    def create_galaxy_table_in_duckdb(  # by default the name of the decorated function is the task_id\n",
    "        duckdb_instance_name: str = _DUCKDB_INSTANCE_NAME,\n",
    "        table_name: str = _DUCKDB_TABLE_NAME,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Create a table in DuckDB to store galaxy data.\n",
    "        This task simulates a setup step in an ETL pipeline.\n",
    "        Args:\n",
    "            duckdb_instance_name: The name of the DuckDB instance.\n",
    "            table_name: The name of the table to be created.\n",
    "        \"\"\"\n",
    "\n",
    "        t_log.info(\"Creating galaxy table in DuckDB.\")\n",
    "\n",
    "        os.makedirs(os.path.dirname(duckdb_instance_name), exist_ok=True)\n",
    "\n",
    "        cursor = duckdb.connect(duckdb_instance_name)\n",
    "\n",
    "        cursor.execute(\n",
    "            f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "                name STRING PRIMARY KEY,\n",
    "                distance_from_milkyway INT,\n",
    "                distance_from_solarsystem INT,\n",
    "                type_of_galaxy STRING,\n",
    "                characteristics STRING\n",
    "            )\"\"\"\n",
    "        )\n",
    "        cursor.close()\n",
    "\n",
    "        t_log.info(f\"Table {table_name} created in DuckDB.\")\n",
    "\n",
    "    @task\n",
    "    def extract_galaxy_data(num_galaxies: int = _NUM_GALAXIES_TOTAL) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Retrieve data about galaxies.\n",
    "        This task simulates an extraction step in an ETL pipeline.\n",
    "        Args:\n",
    "            num_galaxies (int): The number of galaxies for which data should be returned.\n",
    "            Default is 20. Maximum is 20.\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing data about galaxies.\n",
    "        \"\"\"\n",
    "\n",
    "        galaxy_df = get_galaxy_data(num_galaxies)\n",
    "\n",
    "        return galaxy_df\n",
    "\n",
    "    @task\n",
    "    def transform_galaxy_data(galaxy_df: pd.DataFrame, **context):\n",
    "        \"\"\"\n",
    "        Filter the galaxy data based on the distance from the Milky Way.\n",
    "        This task simulates a transformation step in an ETL pipeline.\n",
    "        Args:\n",
    "            closeness_threshold_light_years (int): The threshold for filtering\n",
    "            galaxies based on distance.\n",
    "            Default is 500,000 light years.\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing filtered galaxy data.\n",
    "        \"\"\"\n",
    "\n",
    "        # retrieve param values from the context\n",
    "        closeness_threshold_light_years = context[\"params\"][\n",
    "            _CLOSENESS_THRESHOLD_LY_PARAMETER_NAME\n",
    "        ]\n",
    "\n",
    "        t_log.info(\n",
    "            f\"Filtering for galaxies closer than {closeness_threshold_light_years} light years.\"\n",
    "        )\n",
    "\n",
    "        filtered_galaxy_df = galaxy_df[\n",
    "            galaxy_df[\"distance_from_milkyway\"] < closeness_threshold_light_years\n",
    "        ]\n",
    "\n",
    "        return filtered_galaxy_df\n",
    "\n",
    "    @task(\n",
    "        outlets=[Asset(_DUCKDB_TABLE_URI)]\n",
    "    )  # Define that this task produces updates to an Airflow Dataset.\n",
    "    # Downstream DAGs can be scheduled based on combinations of Dataset updates\n",
    "    # coming from tasks in the same Airflow instance or calls to the Airflow API.\n",
    "    # See: https://www.astronomer.io/docs/learn/airflow-datasets\n",
    "    def load_galaxy_data(\n",
    "        filtered_galaxy_df: pd.DataFrame,\n",
    "        duckdb_instance_name: str = _DUCKDB_INSTANCE_NAME,\n",
    "        table_name: str = _DUCKDB_TABLE_NAME,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Load the filtered galaxy data into a DuckDB database.\n",
    "        This task simulates a loading step in an ETL pipeline.\n",
    "        Args:\n",
    "            filtered_galaxy_df (pd.DataFrame): The filtered galaxy data to be loaded.\n",
    "            duckdb_instance_name (str): The name of the DuckDB instance.\n",
    "            table_name (str): The name of the table to load the data into.\n",
    "        \"\"\"\n",
    "\n",
    "        t_log.info(\"Loading galaxy data into DuckDB.\")\n",
    "        cursor = duckdb.connect(duckdb_instance_name)\n",
    "        cursor.sql(\n",
    "            f\"INSERT OR IGNORE INTO {table_name} BY NAME SELECT * FROM filtered_galaxy_df;\"\n",
    "        )\n",
    "        t_log.info(\"Galaxy data loaded into DuckDB.\")\n",
    "\n",
    "    @task\n",
    "    def print_loaded_galaxies(\n",
    "        duckdb_instance_name: str = _DUCKDB_INSTANCE_NAME,\n",
    "        table_name: str = _DUCKDB_TABLE_NAME,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Get the galaxies stored in the DuckDB database that were filtered\n",
    "        based on closeness to the Milky Way and print them to the logs.\n",
    "        Args:\n",
    "            duck_db_conn_id (str): The connection ID for the duckdb database\n",
    "            where the table is stored.\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing the galaxies closer than\n",
    "            500,000 light years from the Milky Way.\n",
    "        \"\"\"\n",
    "\n",
    "        cursor = duckdb.connect(duckdb_instance_name)\n",
    "        near_galaxies_df = cursor.sql(f\"SELECT * FROM {table_name};\").df()\n",
    "        near_galaxies_df = near_galaxies_df.sort_values(\n",
    "            by=\"distance_from_milkyway\", ascending=True\n",
    "        )\n",
    "        t_log.info(tabulate(near_galaxies_df, headers=\"keys\", tablefmt=\"pretty\"))\n",
    "\n",
    "    # ------------------------------------ #\n",
    "    # Calling tasks + Setting dependencies #\n",
    "    # ------------------------------------ #\n",
    "\n",
    "    # each call of a @task decorated function creates one task in the Airflow UI\n",
    "    # passing the return value of one @task decorated function to another one\n",
    "    # automatically creates a task dependency\n",
    "    create_galaxy_table_in_duckdb_obj = create_galaxy_table_in_duckdb()\n",
    "    extract_galaxy_data_obj = extract_galaxy_data()\n",
    "    transform_galaxy_data_obj = transform_galaxy_data(extract_galaxy_data_obj)\n",
    "    load_galaxy_data_obj = load_galaxy_data(transform_galaxy_data_obj)\n",
    "\n",
    "    # you can set explicit dependencies using the chain function (or bit-shift operators)\n",
    "    # See: https://www.astronomer.io/docs/learn/managing-dependencies\n",
    "    chain(\n",
    "        create_galaxy_table_in_duckdb_obj, load_galaxy_data_obj, print_loaded_galaxies()\n",
    "    )\n",
    "\n",
    "\n",
    "# Instantiate the DAG\n",
    "example_etl_galaxies()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src\\orchestrators\\learning-airflow\\include\\custom_functions\\gtfs_functions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src\\orchestrators\\learning-airflow\\include\\custom_functions\\gtfs_functions.py\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"GTFS helper functions for Airflow DAGs\"\"\"\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class GTFSProcessor:\n",
    "    \"\"\"Class to process GTFS Realtime data\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_data(data):\n",
    "        \"\"\"Process the GTFS data before storing\"\"\"\n",
    "        # Add processing timestamp\n",
    "        processed_data = []\n",
    "        processing_time = datetime.now().isoformat()\n",
    "\n",
    "        for entity in data:\n",
    "            # Add processing metadata\n",
    "            entity['_processing_time'] = processing_time\n",
    "            processed_data.append(entity)\n",
    "\n",
    "        logging.info(f\"Processed {len(processed_data)} GTFS entities\")\n",
    "        return processed_data\n",
    "\n",
    "    @staticmethod\n",
    "    def transform_for_sql(data):\n",
    "        \"\"\"Transform data into a format suitable for SQL insertion\"\"\"\n",
    "        if not data:\n",
    "            return []\n",
    "\n",
    "        sql_ready_data = []\n",
    "        for entity in data:\n",
    "            if 'vehicle' in entity and 'position' in entity['vehicle']:\n",
    "                try:\n",
    "                    vehicle_id = entity.get('id', '') or entity['vehicle'].get('vehicle', {}).get('id', 'unknown')\n",
    "                    position = entity['vehicle']['position']\n",
    "                    timestamp = entity['vehicle'].get('timestamp', '')\n",
    "\n",
    "                    record = (\n",
    "                        vehicle_id,\n",
    "                        position.get('latitude', 0),\n",
    "                        position.get('longitude', 0),\n",
    "                        position.get('bearing', 0),\n",
    "                        position.get('speed', 0),\n",
    "                        timestamp,\n",
    "                        entity.get('_processing_time', '')\n",
    "                    )\n",
    "                    sql_ready_data.append(record)\n",
    "                except (KeyError, TypeError) as e:\n",
    "                    logging.warning(f\"Could not extract position data from entity: {e}\")\n",
    "\n",
    "        logging.info(f\"Transformed {len(sql_ready_data)} entities for SQL insertion\")\n",
    "        # Return as a list of tuples for SQL insertion\n",
    "        return sql_ready_data\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_sql_values(sql_data):\n",
    "        \"\"\"Convert data to SQL VALUES format for PostgresOperator\"\"\"\n",
    "        if not sql_data:\n",
    "            return \"''\"  # Empty string if no data\n",
    "\n",
    "        # Convert list of tuples to SQL VALUES syntax\n",
    "        values_strings = []\n",
    "        for record in sql_data:\n",
    "            values_str = f\"('{record[0]}', {record[1]}, {record[2]}, {record[3]}, {record[4]}, '{record[5]}', '{record[6]}')\"\n",
    "            values_strings.append(values_str)\n",
    "\n",
    "        return \", \".join(values_strings)\n",
    "\n",
    "    @staticmethod\n",
    "    def cleanup_flag_file(flag_file_path):\n",
    "        \"\"\"Clean up the flag file that triggered a DAG\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(flag_file_path):\n",
    "                os.remove(flag_file_path)\n",
    "                logging.info(f\"Removed flag file: {flag_file_path}\")\n",
    "            else:\n",
    "                logging.warning(f\"Flag file not found: {flag_file_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error removing flag file: {e}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src\\orchestrators\\learning-airflow\\dags\\gtfs_data_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src\\orchestrators\\learning-airflow\\dags\\gtfs_data_pipeline.py\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "GTFS Realtime Data Pipeline DAG \n",
    "\n",
    "This DAG fetches GTFS-RT data from the MTA Bus Time API, processes it,\n",
    "and loads it into the storage backend of choice: S3, BigQuery, Azure Blob, or DuckDB.\n",
    "It also demonstrates SQL operations by loading data into PostgreSQL.\n",
    "\n",
    "The DAG demonstrates the Airflow TaskFlow API (Python functions as tasks)\n",
    "and parameterization for different cloud environments.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "from airflow.decorators import dag, task\n",
    "from airflow.models import Variable, Connection\n",
    "from airflow.operators.python import get_current_context\n",
    "\n",
    "from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "\n",
    "# Import directly from the src/ingestion package\n",
    "from include.custom_functions.gtfs_functions import GTFSProcessor\n",
    "\n",
    "# Import custom modularized functions\n",
    "from pendulum import today, duration\n",
    "\n",
    "# Define helper function to replace days_ago\n",
    "def days_ago(n: int):\n",
    "    return today(\"UTC\").subtract(days=n)\n",
    "\n",
    "\n",
    "# Default settings applied to all tasks\n",
    "default_args = {\n",
    "    'owner': 'data-engineering',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'execution_timeout': timedelta(minutes=10),\n",
    "}\n",
    "\n",
    "# Configurable parameters with defaults\n",
    "# These can be overridden by setting Airflow Variables\n",
    "CLOUD_PROVIDER = Variable.get(\"CLOUD_PROVIDER\", default_var=\"local\")  # aws, gcp, azure, or local\n",
    "STORAGE_TYPE = Variable.get(\"STORAGE_TYPE\", default_var=\"duckdb\")  # s3, gcs, azure_blob, bigquery, duckdb\n",
    "API_URL = Variable.get(\"GTFS_API_URL\", default_var=\"https://gtfsrt.prod.obanyc.com/vehiclePositions\")\n",
    "OUTPUT_FORMAT = Variable.get(\"OUTPUT_FORMAT\", default_var=\"json\")\n",
    "USE_SQL_DB = Variable.get(\"USE_SQL_DB\", default_var=\"true\").lower() == \"true\"  # Whether to also load data into PostgreSQL\n",
    "\n",
    "# Cloud-specific settings with defaults\n",
    "if CLOUD_PROVIDER == \"aws\":\n",
    "    S3_BUCKET = Variable.get(\"S3_BUCKET\", default_var=\"gtfs-data\")\n",
    "    S3_PREFIX = Variable.get(\"S3_PREFIX\", default_var=\"vehicle_positions\")\n",
    "elif CLOUD_PROVIDER == \"gcp\":\n",
    "    GCS_BUCKET = Variable.get(\"GCS_BUCKET\", default_var=\"gtfs-data\")\n",
    "    GCS_PREFIX = Variable.get(\"GCS_PREFIX\", default_var=\"vehicle_positions\")\n",
    "    BQ_DATASET = Variable.get(\"BQ_DATASET\", default_var=\"gtfs_data\")\n",
    "    BQ_TABLE = Variable.get(\"BQ_TABLE\", default_var=\"vehicle_positions\")\n",
    "elif CLOUD_PROVIDER == \"azure\":\n",
    "    AZURE_CONTAINER = Variable.get(\"AZURE_CONTAINER\", default_var=\"gtfs-data\")\n",
    "    AZURE_PREFIX = Variable.get(\"AZURE_PREFIX\", default_var=\"vehicle_positions\")\n",
    "else:  # local\n",
    "    DUCKDB_PATH = Variable.get(\"DUCKDB_PATH\", default_var=\"/tmp/gtfs.duckdb\")\n",
    "    DUCKDB_TABLE = Variable.get(\"DUCKDB_TABLE\", default_var=\"vehicle_positions\")\n",
    "\n",
    "# Define an asset for asset-driven scheduling\n",
    "from airflow.sdk import Asset, AssetWatcher\n",
    "from airflow.providers.standard.triggers.file import FileDeleteTrigger\n",
    "\n",
    "# Create a file sensor trigger for the GTFS asset\n",
    "gtfs_file_trigger = FileDeleteTrigger(filepath=\"/data/gtfs/new_data.flag\")\n",
    "gtfs_asset = Asset(\n",
    "    \"gtfs_data_asset\", \n",
    "    watchers=[AssetWatcher(name=\"gtfs_data_watcher\", trigger=gtfs_file_trigger)]\n",
    ")\n",
    "\n",
    "@dag(\n",
    "    default_args=default_args,\n",
    "    schedule=[gtfs_asset],  # asset-driven scheduling\n",
    "    start_date=days_ago(1),\n",
    "    catchup=False,\n",
    "    max_active_runs=1,\n",
    "    dag_display_name=\"GTFS Real-time Data Pipeline 🚌\",\n",
    "    tags=['gtfs', 'realtime', 'sql', CLOUD_PROVIDER],\n",
    "    doc_md=__doc__\n",
    ")\n",
    "def gtfs_data_pipeline():\n",
    "    \"\"\"\n",
    "    ### GTFS-RT Data Pipeline\n",
    "\n",
    "    This DAG demonstrates how to fetch and process GTFS-RT data with Airflow,\n",
    "    using different cloud providers and storage backends.\n",
    "\n",
    "    #### Environment configuration\n",
    "    * Cloud Provider: {cloud_provider}\n",
    "    * Storage Type: {storage_type}\n",
    "    * Data Format: {format}\n",
    "    * Also Load to SQL DB: {use_sql_db}\n",
    "    * Schedule: Asset-driven (file trigger)\n",
    "    \"\"\".format(\n",
    "        cloud_provider=CLOUD_PROVIDER,\n",
    "        storage_type=STORAGE_TYPE,\n",
    "        format=OUTPUT_FORMAT,\n",
    "        use_sql_db=USE_SQL_DB\n",
    "    )\n",
    "\n",
    "    @task()\n",
    "    def fetch_gtfs():\n",
    "        \"\"\"Fetch GTFS-RT data from the configured API\"\"\"\n",
    "        # Get API key from connection if configured\n",
    "        try:\n",
    "            conn = Connection.get_connection_from_secrets(\"gtfs_api\")\n",
    "            api_key = conn.password if conn else Variable.get(\"MTA_API_KEY\", default_var=os.getenv(\"MTA_API_KEY\"))\n",
    "        except:\n",
    "            api_key = Variable.get(\"MTA_API_KEY\", default_var=os.getenv(\"MTA_API_KEY\"))\n",
    "\n",
    "        # Initialize fetcher\n",
    "        fetcher = GTFSFetcher(api_url=API_URL, api_key=api_key)\n",
    "\n",
    "        # Get the data\n",
    "        logging.info(f\"Fetching GTFS data from {API_URL}\")\n",
    "        try:\n",
    "            data = fetcher.fetch_and_parse()\n",
    "            logging.info(f\"Successfully fetched {len(data)} GTFS entities\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching GTFS data: {e}\")\n",
    "            raise\n",
    "\n",
    "    @task()\n",
    "    def process_data(data):\n",
    "        \"\"\"Process the GTFS data before storing\"\"\"\n",
    "        # Use the modularized GTFSProcessor class\n",
    "        return GTFSProcessor.process_data(data)\n",
    "\n",
    "    @task()\n",
    "    def transform_for_sql(data):\n",
    "        \"\"\"Transform data into a format suitable for SQL insertion\"\"\"\n",
    "        # Use the modularized GTFSProcessor class\n",
    "        return GTFSProcessor.transform_for_sql(data)\n",
    "\n",
    "    @task()\n",
    "    def prepare_sql_values(sql_data):\n",
    "        \"\"\"Convert data to SQL VALUES format for SQLExecuteQueryOperator\"\"\"\n",
    "        # Use the modularized GTFSProcessor class\n",
    "        return GTFSProcessor.prepare_sql_values(sql_data)\n",
    "\n",
    "    @task()\n",
    "    def store_data(data):\n",
    "        \"\"\"Store the data in the configured backend\"\"\"\n",
    "        if not data:\n",
    "            logging.warning(\"No data to store\")\n",
    "            return {\"status\": \"warning\", \"message\": \"No data to store\"}\n",
    "\n",
    "        # Get the fetcher for storage methods\n",
    "        try:\n",
    "            conn = Connection.get_connection_from_secrets(\"gtfs_api\")\n",
    "            api_key = conn.password if conn else os.getenv(\"MTA_API_KEY\")\n",
    "        except:\n",
    "            api_key = os.getenv(\"MTA_API_KEY\")\n",
    "\n",
    "        fetcher = GTFSFetcher(api_url=API_URL, api_key=api_key)\n",
    "\n",
    "        # Store based on the configured backend\n",
    "        try:\n",
    "            if CLOUD_PROVIDER == \"aws\":\n",
    "                location = fetcher.save_to_s3(\n",
    "                    data, \n",
    "                    bucket=S3_BUCKET, \n",
    "                    prefix=S3_PREFIX, \n",
    "                    fmt=OUTPUT_FORMAT\n",
    "                )\n",
    "                logging.info(f\"Data saved to S3: {location}\")\n",
    "                return {\"status\": \"success\", \"location\": location}\n",
    "\n",
    "            elif CLOUD_PROVIDER == \"gcp\":\n",
    "                if STORAGE_TYPE == \"bigquery\":\n",
    "                    rows = fetcher.save_to_bigquery(data, BQ_DATASET, BQ_TABLE)\n",
    "                    logging.info(f\"Data saved to BigQuery: {rows} rows\")\n",
    "                    return {\"status\": \"success\", \"rows\": rows}\n",
    "                else:\n",
    "                    location = fetcher.save_to_gcs(\n",
    "                        data, \n",
    "                        bucket=GCS_BUCKET, \n",
    "                        prefix=GCS_PREFIX, \n",
    "                        fmt=OUTPUT_FORMAT\n",
    "                    )\n",
    "                    logging.info(f\"Data saved to GCS: {location}\")\n",
    "                    return {\"status\": \"success\", \"location\": location}\n",
    "\n",
    "            elif CLOUD_PROVIDER == \"azure\":\n",
    "                # Azure implementation would go here\n",
    "                # This would use the Azure blob storage client\n",
    "                logging.info(\"Azure storage not yet implemented\")\n",
    "                return {\"status\": \"not_implemented\", \"message\": \"Azure storage not yet implemented\"}\n",
    "\n",
    "            else:  # local/duckdb\n",
    "                rows = fetcher.save_to_duckdb(data, table=DUCKDB_TABLE, db_path=DUCKDB_PATH)\n",
    "                logging.info(f\"Data saved to DuckDB: {DUCKDB_PATH}, table: {DUCKDB_TABLE}, {rows} rows\")\n",
    "                return {\"status\": \"success\", \"rows\": rows, \"database\": DUCKDB_PATH}\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error storing data: {e}\")\n",
    "            raise\n",
    "\n",
    "    # Create PostgreSQL tables\n",
    "    create_pg_table = SQLExecuteQueryOperator(\n",
    "        task_id=\"create_gtfs_table\",\n",
    "        conn_id=\"postgres_default\",\n",
    "        sql=\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS public.gtfs_vehicle_positions (\n",
    "            vehicle_id TEXT,\n",
    "            latitude DOUBLE PRECISION,\n",
    "            longitude DOUBLE PRECISION,\n",
    "            bearing DOUBLE PRECISION,\n",
    "            speed DOUBLE PRECISION,\n",
    "            timestamp TIMESTAMP,\n",
    "            processing_time TIMESTAMP,\n",
    "            PRIMARY KEY (vehicle_id, processing_time)\n",
    "        );\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Insert task with dynamic SQL\n",
    "    @task()\n",
    "    def insert_to_postgres(values):\n",
    "        \"\"\"Insert the values into PostgreSQL using SQLExecuteQueryOperator\"\"\"\n",
    "        if not values or values == \"''\":\n",
    "            logging.warning(\"No values to insert into PostgreSQL\")\n",
    "            return {\"rows_inserted\": 0}\n",
    "\n",
    "        pg_insert = SQLExecuteQueryOperator(\n",
    "            task_id=\"insert_gtfs_data\",\n",
    "            conn_id=\"postgres_default\",\n",
    "            sql=f\"\"\"\n",
    "            INSERT INTO public.gtfs_vehicle_positions\n",
    "            (vehicle_id, latitude, longitude, bearing, speed, timestamp, processing_time)\n",
    "            VALUES {values}\n",
    "            ON CONFLICT (vehicle_id, processing_time) \n",
    "            DO UPDATE SET\n",
    "                latitude = EXCLUDED.latitude,\n",
    "                longitude = EXCLUDED.longitude,\n",
    "                bearing = EXCLUDED.bearing,\n",
    "                speed = EXCLUDED.speed;\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        pg_insert.execute(context={})\n",
    "        return {\"rows_inserted\": values.count('),') + 1 if values else 0}\n",
    "\n",
    "    # Task to clean up the flag file that triggered this DAG\n",
    "    @task()\n",
    "    def cleanup():\n",
    "        \"\"\"Clean up the flag file that triggered this DAG\"\"\"\n",
    "        # Use the modularized GTFSProcessor class\n",
    "        return GTFSProcessor.cleanup_flag_file(\"/data/gtfs/new_data.flag\")\n",
    "\n",
    "    # Define SQL branch based on configuration\n",
    "    sql_branch = EmptyOperator(task_id=\"skip_sql_branch\") if not USE_SQL_DB else EmptyOperator(task_id=\"use_sql_branch\")\n",
    "\n",
    "    # Define the task dependencies\n",
    "    raw_data = fetch_gtfs()\n",
    "    processed_data = process_data(raw_data)\n",
    "    storage_result = store_data(processed_data)\n",
    "\n",
    "    # SQL branch\n",
    "    if USE_SQL_DB:\n",
    "        sql_data = transform_for_sql(processed_data)\n",
    "        sql_values = prepare_sql_values(sql_data)\n",
    "        create_pg_table >> insert_to_postgres(sql_values) >> cleanup()\n",
    "\n",
    "    # Main flow\n",
    "    raw_data >> processed_data >> storage_result\n",
    "\n",
    "    # Return the DAG result\n",
    "    return {\"result\": storage_result}\n",
    "\n",
    "# Instantiate the DAG\n",
    "gtfs_pipeline = gtfs_data_pipeline() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src\\orchestrators\\learning-airflow\\include\\custom_functions\\nba_functions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src\\orchestrators\\learning-airflow\\include\\custom_functions\\nba_functions.py\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"NBA data processing helper functions for Airflow DAGs\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "\n",
    "\n",
    "class NBAProcessor:\n",
    "    \"\"\"Class to process NBA game data\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def fetch_nba_games() -> list[dict]:\n",
    "        \"\"\"\n",
    "        Fetch today's NBA games live from stats.nba.com via nba_api.\n",
    "        No API key required.\n",
    "        \"\"\"\n",
    "        from datetime import datetime\n",
    "        from nba_api.stats.endpoints import ScoreboardV2\n",
    "\n",
    "        # NBA expects dates as MM/DD/YYYY\n",
    "        game_date = datetime.utcnow().strftime(\"%m/%d/%Y\")\n",
    "        sb = ScoreboardV2(game_date=game_date)\n",
    "        payload = sb.get_dict()\n",
    "        rows = payload[\"resultSets\"][0][\"rowSet\"]\n",
    "\n",
    "        games = []\n",
    "        for row in rows:\n",
    "            games.append({\n",
    "                \"game_id\":       row[2],            # GAME_ID\n",
    "                \"date\":          row[0],            # GAME_DATE\n",
    "                \"home_team\":     row[6],            # HOME_TEAM_ABBREVIATION\n",
    "                \"away_team\":     row[7],            # VISITOR_TEAM_ABBREVIATION\n",
    "                \"score_home\":    row[21] or 0,      # PTS_HOME; if None, 0\n",
    "                \"score_away\":    row[22] or 0,      # PTS_AWAY; if None, 0\n",
    "            })\n",
    "        return games\n",
    "\n",
    "    @staticmethod\n",
    "    def process_games(games_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Process the NBA games data\n",
    "        \n",
    "        Args:\n",
    "            games_data: Raw NBA game data\n",
    "            \n",
    "        Returns:\n",
    "            Processed NBA game data with added timestamps\n",
    "        \"\"\"\n",
    "        # Add processing metadata\n",
    "        processed_data = []\n",
    "        processing_time = datetime.now().isoformat()\n",
    "\n",
    "        for game in games_data:\n",
    "            # Add processing timestamp\n",
    "            game['processing_time'] = processing_time\n",
    "            processed_data.append(game)\n",
    "\n",
    "        logging.info(f\"Processed {len(processed_data)} NBA games\")\n",
    "        return processed_data\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_sql_values(games: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"\n",
    "        Prepare SQL VALUES for insertion\n",
    "        \n",
    "        Args:\n",
    "            games: List of processed game data\n",
    "            \n",
    "        Returns:\n",
    "            SQL VALUES string for insertion\n",
    "        \"\"\"\n",
    "        if not games:\n",
    "            return \"''\"\n",
    "\n",
    "        values_strings = []\n",
    "        for game in games:\n",
    "            # Format values for SQL INSERT\n",
    "            values_str = f\"('{game['id']}', '{game['date']}', '{game['home_team']}', '{game['away_team']}', {game['score_home']}, {game['score_away']}, CURRENT_TIMESTAMP)\"\n",
    "            values_strings.append(values_str)\n",
    "\n",
    "        return \", \".join(values_strings)\n",
    "        \n",
    "    @staticmethod\n",
    "    def cleanup_flag_file(flag_file_path: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Clean up the flag file that triggered a DAG\n",
    "        \n",
    "        Args:\n",
    "            flag_file_path: Path to the flag file\n",
    "            \n",
    "        Returns:\n",
    "            Status dictionary\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if os.path.exists(flag_file_path):\n",
    "                os.remove(flag_file_path)\n",
    "                logging.info(f\"Removed flag file: {flag_file_path}\")\n",
    "            else:\n",
    "                logging.warning(f\"Flag file not found: {flag_file_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error removing flag file: {e}\")\n",
    "            \n",
    "        return {\"status\": \"success\"} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src\\orchestrators\\learning-airflow\\dags\\nba_ingest_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src\\orchestrators\\learning-airflow\\dags\\nba_ingest_pipeline.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "GTFS Realtime Data Pipeline DAG \n",
    "\n",
    "This DAG fetches GTFS-RT data from the MTA Bus Time API, processes it,\n",
    "and loads it into the storage backend of choice: S3, BigQuery, Azure Blob, or DuckDB.\n",
    "It also demonstrates SQL operations by loading data into PostgreSQL.\n",
    "\n",
    "The DAG demonstrates the Airflow TaskFlow API (Python functions as tasks)\n",
    "and parameterization for different cloud environments.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "from airflow.decorators import dag, task\n",
    "from airflow.models import Variable, Connection\n",
    "from airflow.operators.python import get_current_context\n",
    "\n",
    "from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "\n",
    "# Import directly from the src/ingestion package\n",
    "from include.custom_functions.gtfs_functions import GTFSProcessor\n",
    "\n",
    "# Import custom modularized functions\n",
    "from include.custom_functions.gtfs_functions import GTFSProcessor\n",
    "from pendulum import today, duration\n",
    "\n",
    "# Define helper function to replace days_ago\n",
    "def days_ago(n: int):\n",
    "    return today(\"UTC\").subtract(days=n)\n",
    "\n",
    "\n",
    "# Default settings applied to all tasks\n",
    "default_args = {\n",
    "    'owner': 'data-engineering',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'execution_timeout': timedelta(minutes=10),\n",
    "}\n",
    "\n",
    "# Configurable parameters with defaults\n",
    "# These can be overridden by setting Airflow Variables\n",
    "CLOUD_PROVIDER = Variable.get(\"CLOUD_PROVIDER\", default_var=\"local\")  # aws, gcp, azure, or local\n",
    "STORAGE_TYPE = Variable.get(\"STORAGE_TYPE\", default_var=\"duckdb\")  # s3, gcs, azure_blob, bigquery, duckdb\n",
    "API_URL = Variable.get(\"GTFS_API_URL\", default_var=\"https://gtfsrt.prod.obanyc.com/vehiclePositions\")\n",
    "OUTPUT_FORMAT = Variable.get(\"OUTPUT_FORMAT\", default_var=\"json\")\n",
    "USE_SQL_DB = Variable.get(\"USE_SQL_DB\", default_var=\"true\").lower() == \"true\"  # Whether to also load data into PostgreSQL\n",
    "\n",
    "# Cloud-specific settings with defaults\n",
    "if CLOUD_PROVIDER == \"aws\":\n",
    "    S3_BUCKET = Variable.get(\"S3_BUCKET\", default_var=\"gtfs-data\")\n",
    "    S3_PREFIX = Variable.get(\"S3_PREFIX\", default_var=\"vehicle_positions\")\n",
    "elif CLOUD_PROVIDER == \"gcp\":\n",
    "    GCS_BUCKET = Variable.get(\"GCS_BUCKET\", default_var=\"gtfs-data\")\n",
    "    GCS_PREFIX = Variable.get(\"GCS_PREFIX\", default_var=\"vehicle_positions\")\n",
    "    BQ_DATASET = Variable.get(\"BQ_DATASET\", default_var=\"gtfs_data\")\n",
    "    BQ_TABLE = Variable.get(\"BQ_TABLE\", default_var=\"vehicle_positions\")\n",
    "elif CLOUD_PROVIDER == \"azure\":\n",
    "    AZURE_CONTAINER = Variable.get(\"AZURE_CONTAINER\", default_var=\"gtfs-data\")\n",
    "    AZURE_PREFIX = Variable.get(\"AZURE_PREFIX\", default_var=\"vehicle_positions\")\n",
    "else:  # local\n",
    "    DUCKDB_PATH = Variable.get(\"DUCKDB_PATH\", default_var=\"/tmp/gtfs.duckdb\")\n",
    "    DUCKDB_TABLE = Variable.get(\"DUCKDB_TABLE\", default_var=\"vehicle_positions\")\n",
    "\n",
    "# Define an asset for asset-driven scheduling\n",
    "from airflow.sdk import Asset, AssetWatcher\n",
    "from airflow.providers.standard.triggers.file import FileDeleteTrigger\n",
    "\n",
    "# Create a file sensor trigger for the GTFS asset\n",
    "gtfs_file_trigger = FileDeleteTrigger(filepath=\"/data/gtfs/new_data.flag\")\n",
    "gtfs_asset = Asset(\n",
    "    \"gtfs_data_asset\", \n",
    "    watchers=[AssetWatcher(name=\"gtfs_data_watcher\", trigger=gtfs_file_trigger)]\n",
    ")\n",
    "\n",
    "@dag(\n",
    "    default_args=default_args,\n",
    "    schedule=[gtfs_asset],  # asset-driven scheduling\n",
    "    start_date=days_ago(1),\n",
    "    catchup=False,\n",
    "    max_active_runs=1,\n",
    "    dag_display_name=\"GTFS Real-time Data Pipeline 🚌\",\n",
    "    tags=['gtfs', 'realtime', 'sql', CLOUD_PROVIDER],\n",
    "    doc_md=__doc__\n",
    ")\n",
    "def gtfs_data_pipeline():\n",
    "    \"\"\"\n",
    "    ### GTFS-RT Data Pipeline\n",
    "\n",
    "    This DAG demonstrates how to fetch and process GTFS-RT data with Airflow,\n",
    "    using different cloud providers and storage backends.\n",
    "\n",
    "    #### Environment configuration\n",
    "    * Cloud Provider: {cloud_provider}\n",
    "    * Storage Type: {storage_type}\n",
    "    * Data Format: {format}\n",
    "    * Also Load to SQL DB: {use_sql_db}\n",
    "    * Schedule: Asset-driven (file trigger)\n",
    "    \"\"\".format(\n",
    "        cloud_provider=CLOUD_PROVIDER,\n",
    "        storage_type=STORAGE_TYPE,\n",
    "        format=OUTPUT_FORMAT,\n",
    "        use_sql_db=USE_SQL_DB\n",
    "    )\n",
    "\n",
    "    @task()\n",
    "    def fetch_gtfs():\n",
    "        \"\"\"Fetch GTFS-RT data from the configured API\"\"\"\n",
    "        # Get API key from connection if configured\n",
    "        try:\n",
    "            conn = Connection.get_connection_from_secrets(\"gtfs_api\")\n",
    "            api_key = conn.password if conn else Variable.get(\"MTA_API_KEY\", default_var=os.getenv(\"MTA_API_KEY\"))\n",
    "        except:\n",
    "            api_key = Variable.get(\"MTA_API_KEY\", default_var=os.getenv(\"MTA_API_KEY\"))\n",
    "\n",
    "        # Initialize fetcher\n",
    "        fetcher = GTFSFetcher(api_url=API_URL, api_key=api_key)\n",
    "\n",
    "        # Get the data\n",
    "        logging.info(f\"Fetching GTFS data from {API_URL}\")\n",
    "        try:\n",
    "            data = fetcher.fetch_and_parse()\n",
    "            logging.info(f\"Successfully fetched {len(data)} GTFS entities\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching GTFS data: {e}\")\n",
    "            raise\n",
    "\n",
    "    @task()\n",
    "    def process_data(data):\n",
    "        \"\"\"Process the GTFS data before storing\"\"\"\n",
    "        # Use the modularized GTFSProcessor class\n",
    "        return GTFSProcessor.process_data(data)\n",
    "\n",
    "    @task()\n",
    "    def transform_for_sql(data):\n",
    "        \"\"\"Transform data into a format suitable for SQL insertion\"\"\"\n",
    "        # Use the modularized GTFSProcessor class\n",
    "        return GTFSProcessor.transform_for_sql(data)\n",
    "\n",
    "    @task()\n",
    "    def prepare_sql_values(sql_data):\n",
    "        \"\"\"Convert data to SQL VALUES format for SQLExecuteQueryOperator\"\"\"\n",
    "        # Use the modularized GTFSProcessor class\n",
    "        return GTFSProcessor.prepare_sql_values(sql_data)\n",
    "\n",
    "    @task()\n",
    "    def store_data(data):\n",
    "        \"\"\"Store the data in the configured backend\"\"\"\n",
    "        if not data:\n",
    "            logging.warning(\"No data to store\")\n",
    "            return {\"status\": \"warning\", \"message\": \"No data to store\"}\n",
    "\n",
    "        # Get the fetcher for storage methods\n",
    "        try:\n",
    "            conn = Connection.get_connection_from_secrets(\"gtfs_api\")\n",
    "            api_key = conn.password if conn else os.getenv(\"MTA_API_KEY\")\n",
    "        except:\n",
    "            api_key = os.getenv(\"MTA_API_KEY\")\n",
    "\n",
    "        fetcher = GTFSFetcher(api_url=API_URL, api_key=api_key)\n",
    "\n",
    "        # Store based on the configured backend\n",
    "        try:\n",
    "            if CLOUD_PROVIDER == \"aws\":\n",
    "                location = fetcher.save_to_s3(\n",
    "                    data, \n",
    "                    bucket=S3_BUCKET, \n",
    "                    prefix=S3_PREFIX, \n",
    "                    fmt=OUTPUT_FORMAT\n",
    "                )\n",
    "                logging.info(f\"Data saved to S3: {location}\")\n",
    "                return {\"status\": \"success\", \"location\": location}\n",
    "\n",
    "            elif CLOUD_PROVIDER == \"gcp\":\n",
    "                if STORAGE_TYPE == \"bigquery\":\n",
    "                    rows = fetcher.save_to_bigquery(data, BQ_DATASET, BQ_TABLE)\n",
    "                    logging.info(f\"Data saved to BigQuery: {rows} rows\")\n",
    "                    return {\"status\": \"success\", \"rows\": rows}\n",
    "                else:\n",
    "                    location = fetcher.save_to_gcs(\n",
    "                        data, \n",
    "                        bucket=GCS_BUCKET, \n",
    "                        prefix=GCS_PREFIX, \n",
    "                        fmt=OUTPUT_FORMAT\n",
    "                    )\n",
    "                    logging.info(f\"Data saved to GCS: {location}\")\n",
    "                    return {\"status\": \"success\", \"location\": location}\n",
    "\n",
    "            elif CLOUD_PROVIDER == \"azure\":\n",
    "                # Azure implementation would go here\n",
    "                # This would use the Azure blob storage client\n",
    "                logging.info(\"Azure storage not yet implemented\")\n",
    "                return {\"status\": \"not_implemented\", \"message\": \"Azure storage not yet implemented\"}\n",
    "\n",
    "            else:  # local/duckdb\n",
    "                rows = fetcher.save_to_duckdb(data, table=DUCKDB_TABLE, db_path=DUCKDB_PATH)\n",
    "                logging.info(f\"Data saved to DuckDB: {DUCKDB_PATH}, table: {DUCKDB_TABLE}, {rows} rows\")\n",
    "                return {\"status\": \"success\", \"rows\": rows, \"database\": DUCKDB_PATH}\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error storing data: {e}\")\n",
    "            raise\n",
    "\n",
    "    # Create PostgreSQL tables\n",
    "    create_pg_table = SQLExecuteQueryOperator(\n",
    "        task_id=\"create_gtfs_table\",\n",
    "        conn_id=\"postgres_default\",\n",
    "        sql=\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS public.gtfs_vehicle_positions (\n",
    "            vehicle_id TEXT,\n",
    "            latitude DOUBLE PRECISION,\n",
    "            longitude DOUBLE PRECISION,\n",
    "            bearing DOUBLE PRECISION,\n",
    "            speed DOUBLE PRECISION,\n",
    "            timestamp TIMESTAMP,\n",
    "            processing_time TIMESTAMP,\n",
    "            PRIMARY KEY (vehicle_id, processing_time)\n",
    "        );\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Insert task with dynamic SQL\n",
    "    @task()\n",
    "    def insert_to_postgres(values):\n",
    "        \"\"\"Insert the values into PostgreSQL using SQLExecuteQueryOperator\"\"\"\n",
    "        if not values or values == \"''\":\n",
    "            logging.warning(\"No values to insert into PostgreSQL\")\n",
    "            return {\"rows_inserted\": 0}\n",
    "\n",
    "        pg_insert = SQLExecuteQueryOperator(\n",
    "            task_id=\"insert_gtfs_data\",\n",
    "            conn_id=\"postgres_default\",\n",
    "            sql=f\"\"\"\n",
    "            INSERT INTO public.gtfs_vehicle_positions\n",
    "            (vehicle_id, latitude, longitude, bearing, speed, timestamp, processing_time)\n",
    "            VALUES {values}\n",
    "            ON CONFLICT (vehicle_id, processing_time) \n",
    "            DO UPDATE SET\n",
    "                latitude = EXCLUDED.latitude,\n",
    "                longitude = EXCLUDED.longitude,\n",
    "                bearing = EXCLUDED.bearing,\n",
    "                speed = EXCLUDED.speed;\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        pg_insert.execute(context={})\n",
    "        return {\"rows_inserted\": values.count('),') + 1 if values else 0}\n",
    "\n",
    "    # Task to clean up the flag file that triggered this DAG\n",
    "    @task()\n",
    "    def cleanup():\n",
    "        \"\"\"Clean up the flag file that triggered this DAG\"\"\"\n",
    "        # Use the modularized GTFSProcessor class\n",
    "        return GTFSProcessor.cleanup_flag_file(\"/data/gtfs/new_data.flag\")\n",
    "\n",
    "    # Define SQL branch based on configuration\n",
    "    sql_branch = EmptyOperator(task_id=\"skip_sql_branch\") if not USE_SQL_DB else EmptyOperator(task_id=\"use_sql_branch\")\n",
    "\n",
    "    # Define the task dependencies\n",
    "    raw_data = fetch_gtfs()\n",
    "    processed_data = process_data(raw_data)\n",
    "    storage_result = store_data(processed_data)\n",
    "\n",
    "    # SQL branch\n",
    "    if USE_SQL_DB:\n",
    "        sql_data = transform_for_sql(processed_data)\n",
    "        sql_values = prepare_sql_values(sql_data)\n",
    "        create_pg_table >> insert_to_postgres(sql_values) >> cleanup()\n",
    "\n",
    "    # Main flow\n",
    "    raw_data >> processed_data >> storage_result\n",
    "\n",
    "    # Return the DAG result\n",
    "    return {\"result\": storage_result}\n",
    "\n",
    "# Instantiate the DAG\n",
    "gtfs_pipeline = gtfs_data_pipeline() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src\\orchestrators\\learning-airflow\\include\\custom_functions\\weather_functions.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "%%writefile src\\orchestrators\\learning-airflow\\include\\custom_functions\\weather_functions.py\n",
    "\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"Weather data processing helper functions for Airflow DAGs\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "class WeatherProcessor:\n",
    "    \"\"\"Class to process weather data from Kafka\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def consume_kafka_messages(\n",
    "        consumer,\n",
    "        topic: str = \"weather-updates\",\n",
    "        max_messages: int = 100\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Consume weather data from Kafka\n",
    "\n",
    "        Args:\n",
    "            consumer: Kafka consumer instance\n",
    "            topic: Kafka topic to consume from\n",
    "            max_messages: Maximum number of messages to consume\n",
    "\n",
    "        Returns:\n",
    "            List of consumed messages\n",
    "        \"\"\"\n",
    "        messages = []\n",
    "        message_count = 0\n",
    "\n",
    "        logging.info(f\"Starting to consume messages from Kafka topic: {topic}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Use poll to get better control over consumption\n",
    "        try:\n",
    "            while message_count < max_messages:\n",
    "                poll_result = consumer.poll(timeout_ms=5000, max_records=max_messages)\n",
    "                if not poll_result:\n",
    "                    break\n",
    "\n",
    "                # Process all partitions and messages\n",
    "                for tp, records in poll_result.items():\n",
    "                    for record in records:\n",
    "                        try:\n",
    "                            message = json.loads(record.value.decode('utf-8'))\n",
    "                            message['_metadata'] = {\n",
    "                                'topic': record.topic,\n",
    "                                'partition': record.partition,\n",
    "                                'offset': record.offset,\n",
    "                                'timestamp': record.timestamp\n",
    "                            }\n",
    "                            messages.append(message)\n",
    "                            message_count += 1\n",
    "                        except json.JSONDecodeError:\n",
    "                            logging.warning(\n",
    "                                f\"Skipping non-JSON message: {record.value}\"\n",
    "                            )\n",
    "\n",
    "                # Commit offsets after processing\n",
    "                consumer.commit()\n",
    "\n",
    "            logging.info(\n",
    "                f\"Consumed {message_count} messages in \"\n",
    "                f\"{time.time() - start_time:.2f} seconds\"\n",
    "            )\n",
    "        finally:\n",
    "            consumer.close()\n",
    "\n",
    "        return messages\n",
    "\n",
    "    @staticmethod\n",
    "    def process_weather_data(messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Process and transform the weather data\n",
    "\n",
    "        Args:\n",
    "            messages: Raw Kafka messages\n",
    "\n",
    "        Returns:\n",
    "            Processed weather data\n",
    "        \"\"\"\n",
    "        if not messages:\n",
    "            logging.warning(\"No weather messages to process\")\n",
    "            return []\n",
    "\n",
    "        processed_data = []\n",
    "        processing_time = datetime.now().isoformat()\n",
    "\n",
    "        for message in messages:\n",
    "            try:\n",
    "                # Extract weather observation data\n",
    "                observation = {\n",
    "                    'location': message.get('location', 'unknown'),\n",
    "                    'latitude': message.get('lat'),\n",
    "                    'longitude': message.get('lon'),\n",
    "                    'obs_time': message.get('observation_time'),\n",
    "                    'temperature': message.get('temp_c'),\n",
    "                    'humidity': message.get('humidity'),\n",
    "                    'pressure': message.get('pressure_mb'),\n",
    "                    'wind_speed': message.get('wind_kph'),\n",
    "                    'wind_direction': message.get('wind_dir'),\n",
    "                    'conditions': message.get('condition', {}).get('text'),\n",
    "                    '_processing_time': processing_time\n",
    "                }\n",
    "                processed_data.append(observation)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing weather message: {e}\")\n",
    "\n",
    "        logging.info(f\"Processed {len(processed_data)} weather observations\")\n",
    "        return processed_data\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_sql_values(observations: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"\n",
    "        Prepare SQL VALUES for insertion\n",
    "\n",
    "        Args:\n",
    "            observations: List of processed weather observations\n",
    "\n",
    "        Returns:\n",
    "            SQL VALUES string for insertion\n",
    "        \"\"\"\n",
    "        if not observations:\n",
    "            return \"''\"\n",
    "\n",
    "        values_strings = []\n",
    "        for obs in observations:\n",
    "            # Format values for SQL INSERT\n",
    "            values_str = f\"\"\"(\n",
    "                '{obs['location']}', \n",
    "                {obs['latitude'] if obs['latitude'] is not None else 'NULL'}, \n",
    "                {obs['longitude'] if obs['longitude'] is not None else 'NULL'}, \n",
    "                '{obs['obs_time']}', \n",
    "                {obs['temperature'] if obs['temperature'] is not None else 'NULL'}, \n",
    "                {obs['humidity'] if obs['humidity'] is not None else 'NULL'}, \n",
    "                {obs['pressure'] if obs['pressure'] is not None else 'NULL'}, \n",
    "                {obs['wind_speed'] if obs['wind_speed'] is not None else 'NULL'}, \n",
    "                '{obs['wind_direction']}', \n",
    "                '{obs['conditions'] if obs['conditions'] else ''}'\n",
    "            )\"\"\"\n",
    "            values_strings.append(values_str)\n",
    "\n",
    "        return \", \".join(values_strings)\n",
    "\n",
    "    @staticmethod\n",
    "    def cleanup_flag_file(flag_file_path: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Clean up the flag file that triggered a DAG\n",
    "\n",
    "        Args:\n",
    "            flag_file_path: Path to the flag file\n",
    "\n",
    "        Returns:\n",
    "            Status dictionary\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if os.path.exists(flag_file_path):\n",
    "                os.remove(flag_file_path)\n",
    "                logging.info(f\"Removed flag file: {flag_file_path}\")\n",
    "            else:\n",
    "                logging.warning(f\"Flag file not found: {flag_file_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error removing flag file: {e}\")\n",
    "\n",
    "        return {\"status\": \"success\"} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src\\orchestrators\\learning-airflow\\dags\\weather_kafka_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src\\orchestrators\\learning-airflow\\dags\\weather_kafka_pipeline.py\n",
    "from airflow.decorators import dag, task\n",
    "from airflow.models import Variable\n",
    "from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\n",
    "from airflow.providers.apache.kafka.hooks.consume import KafkaConsumerHook  # fixed import\n",
    "from pendulum import datetime\n",
    "\n",
    "from include.custom_functions.weather_functions import WeatherProcessor\n",
    "\n",
    "@dag(\n",
    "    start_date=datetime(2025, 5, 1),\n",
    "    catchup=False,\n",
    "    dag_display_name=\"Weather Kafka Pipeline ☁️\",\n",
    "    tags=['weather', 'kafka', 'event-driven', 'sql']\n",
    ")\n",
    "def weather_kafka_pipeline():\n",
    "    @task()\n",
    "    def consume_kafka_messages():\n",
    "        topic = Variable.get(\"WEATHER_KAFKA_TOPIC\", default_var=\"weather-updates\")\n",
    "        max_messages = int(Variable.get(\"WEATHER_MAX_MESSAGES\", default_var=\"100\"))\n",
    "\n",
    "        kafka_hook = KafkaConsumerHook(\n",
    "            topics=[topic],\n",
    "            kafka_config_id=\"kafka_default\",\n",
    "        )\n",
    "        consumer = kafka_hook.get_consumer()\n",
    "        return WeatherProcessor.consume_kafka_messages(consumer, topic, max_messages)\n",
    "\n",
    "    @task()\n",
    "    def process_weather_data(messages):\n",
    "        return WeatherProcessor.process_weather_data(messages)\n",
    "\n",
    "    @task()\n",
    "    def prepare_sql_values(observations):\n",
    "        return WeatherProcessor.prepare_sql_values(observations)\n",
    "\n",
    "    create_table = SQLExecuteQueryOperator(\n",
    "        task_id=\"create_weather_observations_table\",\n",
    "        conn_id=\"postgres_default\",\n",
    "        sql=\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS weather.observations (\n",
    "          id SERIAL PRIMARY KEY,\n",
    "          location TEXT,\n",
    "          latitude DOUBLE PRECISION,\n",
    "          longitude DOUBLE PRECISION,\n",
    "          obs_time TIMESTAMP,\n",
    "          temperature DOUBLE PRECISION,\n",
    "          humidity DOUBLE PRECISION,\n",
    "          pressure DOUBLE PRECISION,\n",
    "          wind_speed DOUBLE PRECISION,\n",
    "          wind_direction TEXT,\n",
    "          conditions TEXT,\n",
    "          created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    insert = SQLExecuteQueryOperator(\n",
    "        task_id=\"insert_weather_data\",\n",
    "        conn_id=\"postgres_default\",\n",
    "        sql=\"\"\"\n",
    "        INSERT INTO weather.observations\n",
    "        (location, latitude, longitude, obs_time, temperature, humidity, pressure, wind_speed, wind_direction, conditions)\n",
    "        VALUES {{ ti.xcom_pull('prepare_sql_values') }};\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    msgs = consume_kafka_messages()\n",
    "    proc = process_weather_data(msgs)\n",
    "    vals = prepare_sql_values(proc)\n",
    "    create_table >> insert\n",
    "\n",
    "weather_kafka_pipeline_dag = weather_kafka_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src\\orchestrators\\learning-airflow\\dags\\user_metrics_etl.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src\\orchestrators\\learning-airflow\\dags\\user_metrics_etl.py\n",
    "# src/orchestrators/learning-airflow/dags/user_metrics_etl.py\n",
    "\n",
    "from airflow.decorators import dag, task\n",
    "from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\n",
    "from pendulum import datetime  # use pendulum directly\n",
    "\n",
    "from include.custom_functions.user_metrics_functions import UserMetricsProcessor\n",
    "\n",
    "@dag(\n",
    "    start_date=datetime(2025, 5, 1),      # use pendulum directly here\n",
    "    schedule=\"@daily\",\n",
    "    dag_display_name=\"User Metrics ETL 📊\",\n",
    "    catchup=False,\n",
    "    tags=[\"metrics\", \"sql\"]\n",
    ")\n",
    "def user_metrics_etl():\n",
    "    @task()\n",
    "    def extract():\n",
    "        return UserMetricsProcessor.extract()\n",
    "\n",
    "    @task()\n",
    "    def transform(data):\n",
    "        return UserMetricsProcessor.transform(data)\n",
    "\n",
    "    create_table = SQLExecuteQueryOperator(\n",
    "        task_id=\"create_user_metrics_table\",\n",
    "        conn_id=\"postgres_default\",\n",
    "        sql=\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS user_metrics (\n",
    "            user_id INTEGER PRIMARY KEY,\n",
    "            session_count INTEGER,\n",
    "            total_duration_mins INTEGER,\n",
    "            conversion_rate DOUBLE PRECISION,\n",
    "            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    load = SQLExecuteQueryOperator(\n",
    "        task_id=\"load_user_metrics\",\n",
    "        conn_id=\"postgres_default\",\n",
    "        sql=\"\"\"\n",
    "        INSERT INTO user_metrics (user_id, session_count, total_duration_mins, conversion_rate)\n",
    "        VALUES {{ ti.xcom_pull('transform') }}\n",
    "        ON CONFLICT (user_id) DO UPDATE\n",
    "          SET session_count       = EXCLUDED.session_count,\n",
    "              total_duration_mins = EXCLUDED.total_duration_mins,\n",
    "              conversion_rate     = EXCLUDED.conversion_rate,\n",
    "              updated_at          = CURRENT_TIMESTAMP;\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    data = extract()\n",
    "    vals = transform(data)\n",
    "    create_table >> load\n",
    "\n",
    "user_metrics_etl_dag = user_metrics_etl()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
